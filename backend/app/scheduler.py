import logging
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from app.crawlers.crawler_manager import CrawlerManager
from app.crawlers.kis_history_crawler import kis_history_crawler
from app.config import settings
from datetime import datetime
import pytz

logger = logging.getLogger(__name__)

class StockScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.crawler_manager = CrawlerManager()
        self._setup_jobs()

    def _setup_jobs(self):
        """ìŠ¤ì¼€ì¤„ ì‘ì—… ì„¤ì •"""
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        kst = pytz.timezone('Asia/Seoul')

        if settings.ENABLE_AUTO_HISTORY_COLLECTION:
            logger.info("âœ… Auto history collection ENABLED")

            # í‰ì¼ ì˜¤í›„ 4ì‹œ 10ë¶„: í•œêµ­ ì¥ ë§ˆê° í›„ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
            self.scheduler.add_job(
                func=self._collect_tagged_stocks_history,
                trigger=CronTrigger(hour=16, minute=10, day_of_week='mon-fri', timezone=kst),
                id='kr_market_history_collection',
                name='Korean Market History Collection (After Close)',
                replace_existing=True
            )
            logger.info("ğŸ“… Scheduled: Korean market history collection (Mon-Fri 16:10 KST)")

            # í‰ì¼ ì˜¤ì „ 6ì‹œ 10ë¶„ (KST): ë¯¸êµ­ ì¥ ë§ˆê° í›„ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
            # ë¯¸êµ­ ì‹œê°„ ê¸°ì¤€ ì „ë‚  ì˜¤í›„ 5ì‹œ ë§ˆê° (EST: UTC-5) â†’ KST ì˜¤ì „ 7ì‹œ
            # ì•½ê°„ ì—¬ìœ ë¥¼ ë‘¬ì„œ ì˜¤ì „ 6ì‹œ 10ë¶„ì— ìˆ˜ì§‘
            self.scheduler.add_job(
                func=self._collect_tagged_stocks_history,
                trigger=CronTrigger(hour=6, minute=10, day_of_week='tue-sat', timezone=kst),
                id='us_market_history_collection',
                name='US Market History Collection (After Close)',
                replace_existing=True
            )
            logger.info("ğŸ“… Scheduled: US market history collection (Tue-Sat 06:10 KST)")

        else:
            logger.info("âš ï¸ Auto history collection DISABLED - using manual updates only")
            logger.info("ğŸ’¡ To enable: Set ENABLE_AUTO_HISTORY_COLLECTION=true in .env")

    def _crawl_all_stocks(self):
        """ëª¨ë“  ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§"""
        try:
            logger.info("Starting scheduled stock crawling...")
            result = self.crawler_manager.update_stock_list("ALL")
            logger.info(f"Scheduled crawling completed: {result['success']}/{result['total']} stocks")
            return result
        except Exception as e:
            logger.error(f"Error in scheduled stock crawling: {str(e)}")
            return {"success": 0, "failed": 0, "total": 0, "error": str(e)}

    def _collect_tagged_stocks_history(self):
        """íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ (KIS API ì‚¬ìš©) - ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë™ì‘"""
        try:
            mode = settings.HISTORY_COLLECTION_MODE.lower()
            logger.info(f"ğŸš€ Starting scheduled history collection (mode: {mode})...")

            if mode == "tagged":
                # íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©ë§Œ
                result = kis_history_crawler.collect_history_for_tagged_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS
                )
            elif mode == "top":
                # ì‹œì´ ìƒìœ„ Nê°œ ì¢…ëª©
                result = kis_history_crawler.collect_history_for_all_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS,
                    limit=settings.HISTORY_COLLECTION_LIMIT
                )
            else:  # "all" ë˜ëŠ” ê¸°íƒ€
                # ëª¨ë“  í™œì„± ì¢…ëª©
                result = kis_history_crawler.collect_history_for_all_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS,
                    limit=None
                )

            if result.get("success_count", 0) > 0:
                logger.info(
                    f"âœ… History collection completed ({mode} mode): "
                    f"{result['success_count']}/{result['total_stocks']} stocks, "
                    f"{result['total_records']} records saved"
                )
            else:
                logger.warning(
                    f"âš ï¸ History collection completed with errors ({mode} mode): "
                    f"{result.get('success_count', 0)}/{result.get('total_stocks', 0)} stocks"
                )

            return result
        except Exception as e:
            logger.error(f"âŒ Error in scheduled history collection: {str(e)}")
            return {"success_count": 0, "total_stocks": 0, "total_records": 0, "error": str(e)}

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if not self.scheduler.running:
            self.scheduler.start()
            logger.info("Stock scheduler started")

    def stop(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("Stock scheduler stopped")

    def get_jobs(self):
        """í˜„ì¬ ë“±ë¡ëœ ì‘ì—… ëª©ë¡ ë°˜í™˜"""
        jobs = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                'id': job.id,
                'name': job.name,
                'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None,
                'trigger': str(job.trigger)
            })
        return jobs

    def trigger_manual_crawl(self):
        """ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("Manual stock crawling triggered")
        return self._crawl_all_stocks()

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
stock_scheduler = StockScheduler()