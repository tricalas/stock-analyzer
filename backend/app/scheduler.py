import logging
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from app.crawlers.crawler_manager import CrawlerManager
from app.crawlers.kis_history_crawler import kis_history_crawler
from app.config import settings
from datetime import datetime
import pytz

# signal_analyzerëŠ” ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import

logger = logging.getLogger(__name__)

class StockScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.crawler_manager = CrawlerManager()
        self._setup_jobs()

    def _setup_jobs(self):
        """ìŠ¤ì¼€ì¤„ ì‘ì—… ì„¤ì •"""
        # í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
        kst = pytz.timezone('Asia/Seoul')

        if settings.ENABLE_AUTO_HISTORY_COLLECTION:
            logger.info("âœ… Auto history collection ENABLED")

            # í‰ì¼ ì˜¤í›„ 4ì‹œ 10ë¶„: í•œêµ­ ì¥ ë§ˆê° í›„ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
            self.scheduler.add_job(
                func=self._collect_tagged_stocks_history,
                trigger=CronTrigger(hour=16, minute=10, day_of_week='mon-fri', timezone=kst),
                id='kr_market_history_collection',
                name='Korean Market History Collection (After Close)',
                replace_existing=True
            )
            logger.info("ğŸ“… Scheduled: Korean market history collection (Mon-Fri 16:10 KST)")

            # í‰ì¼ ì˜¤ì „ 6ì‹œ 10ë¶„ (KST): ë¯¸êµ­ ì¥ ë§ˆê° í›„ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
            # ë¯¸êµ­ ì‹œê°„ ê¸°ì¤€ ì „ë‚  ì˜¤í›„ 5ì‹œ ë§ˆê° (EST: UTC-5) â†’ KST ì˜¤ì „ 7ì‹œ
            # ì•½ê°„ ì—¬ìœ ë¥¼ ë‘¬ì„œ ì˜¤ì „ 6ì‹œ 10ë¶„ì— ìˆ˜ì§‘
            self.scheduler.add_job(
                func=self._collect_tagged_stocks_history,
                trigger=CronTrigger(hour=6, minute=10, day_of_week='tue-sat', timezone=kst),
                id='us_market_history_collection',
                name='US Market History Collection (After Close)',
                replace_existing=True
            )
            logger.info("ğŸ“… Scheduled: US market history collection (Tue-Sat 06:10 KST)")

            # ì‹œê·¸ë„ ë¶„ì„ ì‘ì—…: íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ í›„ ì•½ 50ë¶„ ë’¤ ì‹¤í–‰
            # í•œêµ­ ì¥: 17:00 (íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ 16:10 + 50ë¶„)
            self.scheduler.add_job(
                func=self._analyze_signals,
                trigger=CronTrigger(hour=17, minute=0, day_of_week='mon-fri', timezone=kst),
                id='kr_market_signal_analysis',
                name='Korean Market Signal Analysis',
                replace_existing=True
            )
            logger.info("ğŸ“Š Scheduled: Korean market signal analysis (Mon-Fri 17:00 KST)")

            # ë¯¸êµ­ ì¥: 07:00 (íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ 06:10 + 50ë¶„)
            self.scheduler.add_job(
                func=self._analyze_signals,
                trigger=CronTrigger(hour=7, minute=0, day_of_week='tue-sat', timezone=kst),
                id='us_market_signal_analysis',
                name='US Market Signal Analysis',
                replace_existing=True
            )
            logger.info("ğŸ“Š Scheduled: US market signal analysis (Tue-Sat 07:00 KST)")

        else:
            logger.info("âš ï¸ Auto history collection DISABLED - using manual updates only")
            logger.info("ğŸ’¡ To enable: Set ENABLE_AUTO_HISTORY_COLLECTION=true in .env")

    def _crawl_all_stocks(self):
        """ëª¨ë“  ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§"""
        try:
            logger.info("Starting scheduled stock crawling...")
            result = self.crawler_manager.update_stock_list("ALL")
            logger.info(f"Scheduled crawling completed: {result['success']}/{result['total']} stocks")
            return result
        except Exception as e:
            logger.error(f"Error in scheduled stock crawling: {str(e)}")
            return {"success": 0, "failed": 0, "total": 0, "error": str(e)}

    def _collect_tagged_stocks_history(self):
        """íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ (KIS API ì‚¬ìš©, ë³‘ë ¬ ì²˜ë¦¬) - ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë™ì‘"""
        try:
            mode = settings.HISTORY_COLLECTION_MODE.lower()
            workers = settings.HISTORY_COLLECTION_WORKERS
            logger.info(f"ğŸš€ Starting scheduled history collection (mode: {mode}, workers: {workers})...")

            if mode == "tagged":
                # íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©ë§Œ
                result = kis_history_crawler.collect_history_for_tagged_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS,
                    max_workers=workers
                )
            elif mode == "top":
                # ì‹œì´ ìƒìœ„ Nê°œ ì¢…ëª©
                result = kis_history_crawler.collect_history_for_all_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS,
                    limit=settings.HISTORY_COLLECTION_LIMIT,
                    max_workers=workers
                )
            else:  # "all" ë˜ëŠ” ê¸°íƒ€
                # ëª¨ë“  í™œì„± ì¢…ëª©
                result = kis_history_crawler.collect_history_for_all_stocks(
                    days=settings.HISTORY_COLLECTION_DAYS,
                    limit=None,
                    max_workers=workers
                )

            if result.get("success_count", 0) > 0:
                logger.info(
                    f"âœ… History collection completed ({mode} mode, {workers} workers): "
                    f"{result['success_count']}/{result['total_stocks']} stocks, "
                    f"skipped: {result.get('skipped', 0)}, "
                    f"{result['total_records']} records saved"
                )
            else:
                logger.warning(
                    f"âš ï¸ History collection completed with errors ({mode} mode): "
                    f"{result.get('success_count', 0)}/{result.get('total_stocks', 0)} stocks"
                )

            return result
        except Exception as e:
            logger.error(f"âŒ Error in scheduled history collection: {str(e)}")
            return {"success_count": 0, "total_stocks": 0, "total_records": 0, "error": str(e)}

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if not self.scheduler.running:
            self.scheduler.start()
            logger.info("Stock scheduler started")

    def stop(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("Stock scheduler stopped")

    def get_jobs(self):
        """í˜„ì¬ ë“±ë¡ëœ ì‘ì—… ëª©ë¡ ë°˜í™˜"""
        jobs = []
        for job in self.scheduler.get_jobs():
            jobs.append({
                'id': job.id,
                'name': job.name,
                'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None,
                'trigger': str(job.trigger)
            })
        return jobs

    def trigger_manual_crawl(self):
        """ìˆ˜ë™ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("Manual stock crawling triggered")
        return self._crawl_all_stocks()

    def trigger_manual_history_collection(self, days: int = None, max_workers: int = None):
        """ìˆ˜ë™ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)"""
        effective_days = days or settings.HISTORY_COLLECTION_DAYS
        effective_workers = max_workers or settings.HISTORY_COLLECTION_WORKERS
        logger.info(f"Manual history collection triggered (days: {effective_days}, workers: {effective_workers})")

        # ì„ì‹œë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        original_days = settings.HISTORY_COLLECTION_DAYS
        original_workers = settings.HISTORY_COLLECTION_WORKERS
        if days is not None:
            settings.HISTORY_COLLECTION_DAYS = days
        if max_workers is not None:
            settings.HISTORY_COLLECTION_WORKERS = max_workers

        try:
            result = self._collect_tagged_stocks_history()
            return result
        finally:
            # ì›ë˜ ê°’ ë³µêµ¬
            settings.HISTORY_COLLECTION_DAYS = original_days
            settings.HISTORY_COLLECTION_WORKERS = original_workers

    def _analyze_signals(self):
        """ì‹œê·¸ë„ ë¶„ì„ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ìš©)"""
        try:
            # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ import
            from app.signal_analyzer import signal_analyzer

            mode = settings.HISTORY_COLLECTION_MODE.lower()
            logger.info(f"ğŸ“Š Starting scheduled signal analysis (mode: {mode})...")

            result = signal_analyzer.analyze_and_store_signals(
                mode=mode,
                limit=settings.HISTORY_COLLECTION_LIMIT if mode == "top" else None,
                days=120  # ì‹œê·¸ë„ ë¶„ì„ì€ í•­ìƒ 120ì¼
            )

            if result.get("stocks_with_signals", 0) > 0:
                logger.info(
                    f"âœ… Signal analysis completed ({mode} mode): "
                    f"{result['stocks_with_signals']}/{result['total_stocks']} stocks, "
                    f"{result['total_signals_saved']} signals saved"
                )
            else:
                logger.warning(f"âš ï¸ Signal analysis completed with no signals found ({mode} mode)")

            return result
        except Exception as e:
            logger.error(f"âŒ Error in scheduled signal analysis: {str(e)}")
            return {"error": str(e)}

# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
stock_scheduler = StockScheduler()