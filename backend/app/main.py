from fastapi import FastAPI, Depends, HTTPException, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import ORJSONResponse
from starlette.middleware.gzip import GZipMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import desc, case, text
from typing import List, Optional
from datetime import datetime, date, timedelta
import logging
import hashlib
import json
from cachetools import TTLCache
import redis
import orjson

from app.config import settings
from app.database import engine, Base, get_db
from app.models import Stock, StockPrice, StockDailyData, StockPriceHistory, StockTag, StockTagAssignment, User, StockSignal, TaskProgress, HistoryCollectionLog
from app import schemas
from app.crawlers.crawler_manager import CrawlerManager
from app.crawlers.price_history_crawler import price_history_crawler
from app.scheduler import stock_scheduler
from app.constants import ETF_KEYWORDS
from app.auth import get_pin_hash, verify_pin, create_access_token, get_current_user, get_optional_current_user
from app.signal_analyzer import signal_analyzer
from app.tasks import collect_history_task, analyze_signals_task, retry_failed_stocks_task
from app.celery_app import celery_app
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis ìºì‹œ ì„¤ì •
try:
    redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)
    redis_client.ping()
    logger.info("âœ… Redis connected successfully")
    USE_REDIS = True
except Exception as e:
    logger.warning(f"âš ï¸ Redis connection failed: {e}. Falling back to memory cache.")
    USE_REDIS = False
    # ë©”ëª¨ë¦¬ ìºì‹œ í´ë°± (TTL 300ì´ˆë¡œ ì¦ê°€)
    stocks_cache = TTLCache(maxsize=1000, ttl=300)

def get_cache(key: str):
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if USE_REDIS:
        try:
            data = redis_client.get(f"stocks:{key}")
            if data:
                return orjson.loads(data)
            return None
        except Exception as e:
            logger.error(f"âŒ Redis get failed: {e}")
            return None
    else:
        return stocks_cache.get(key)

def set_cache(key: str, value: dict, ttl: int = 300):
    """ìºì‹œì— ë°ì´í„° ì €ì¥ (TTL: ê¸°ë³¸ 300ì´ˆ)"""
    if USE_REDIS:
        try:
            # orjson.dumps returns bytes, perfect for Redis
            # OPT_SERIALIZE_NUMPY handles numpy types, OPT_PASSTHROUGH_DATETIME handles datetime
            redis_client.setex(f"stocks:{key}", ttl, orjson.dumps(value, default=str))
        except Exception as e:
            logger.error(f"âŒ Redis set failed: {e}")
    else:
        stocks_cache[key] = value

def invalidate_cache():
    """ëª¨ë“  ìºì‹œë¥¼ ë¬´íš¨í™” (íƒœê·¸ ë³€ê²½ ì‹œ í˜¸ì¶œ)"""
    if USE_REDIS:
        try:
            # Redisì˜ ëª¨ë“  stocks ê´€ë ¨ ìºì‹œ í‚¤ ì‚­ì œ
            for key in redis_client.scan_iter("stocks:*"):
                redis_client.delete(key)
            logger.info("âœ… Redis cache cleared")
        except Exception as e:
            logger.error(f"âŒ Redis cache clear failed: {e}")
    else:
        stocks_cache.clear()
        logger.info("âœ… Memory cache cleared")

Base.metadata.create_all(bind=engine)

# orjsonì„ ê¸°ë³¸ JSON serializerë¡œ ì‚¬ìš© (2-3ë°° ë¹ ë¦„)
app = FastAPI(
    title="Stock Analyzer API",
    version="1.0.0",
    default_response_class=ORJSONResponse
)

# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´ (ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì†ë„ 2-3ë°° í–¥ìƒ)
app.add_middleware(GZipMiddleware, minimum_size=1000)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

crawler_manager = CrawlerManager()

# ì„œë²„ ì‹œì‘ ì‹œ ìºì‹œ í´ë¦¬ì–´ (ë°°í¬ í›„ ìƒˆ ë°ì´í„° ë°˜ì˜)
@app.on_event("startup")
async def startup_event():
    invalidate_cache()
    logger.info("ğŸš€ Server started, cache cleared")

# í¬ë¡¤ë§ ì¿¨íƒ€ì„ ê´€ë¦¬ (10ë¶„)
last_crawl_time = None
CRAWL_COOLDOWN_MINUTES = 10

# ê¸°ë³¸ íƒœê·¸ ì‹œë”©
def seed_default_tags(db: Session):
    """ê¸°ë³¸ íƒœê·¸ ë°ì´í„° ìƒì„± (ì‹œìŠ¤í…œ íƒœê·¸ëŠ” user_token=None)"""
    default_tags = [
        {
            "name": "favorite",
            "display_name": "ê´€ì‹¬",
            "color": "primary",
            "icon": "Star",
            "order": 0,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "dislike",
            "display_name": "ì œì™¸",
            "color": "loss",
            "icon": "ThumbsDown",
            "order": 99,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "owned",
            "display_name": "ë³´ìœ ",
            "color": "gain",
            "icon": "ShoppingCart",
            "order": 1,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "recommended",
            "display_name": "ì¶”ì²œ",
            "color": "primary",
            "icon": "ThumbsUp",
            "order": 2,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "watching",
            "display_name": "ê´€ì°°",
            "color": "muted",
            "icon": "Eye",
            "order": 3,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "error",
            "display_name": "ì—ëŸ¬",
            "color": "loss",
            "icon": "AlertCircle",
            "order": 98,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        }
    ]

    # íƒœê·¸ê°€ í•˜ë‚˜ë„ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ íƒœê·¸ ìƒì„± (ìµœì´ˆ 1íšŒ)
    existing_tags_count = db.query(StockTag).count()
    if existing_tags_count > 0:
        logger.info(f"Tags already exist ({existing_tags_count}), skipping seed")
        return

    for tag_data in default_tags:
        tag = StockTag(**tag_data)
        db.add(tag)
        logger.info(f"Created default tag: {tag_data['display_name']}")

    db.commit()
    logger.info("Default tags seeded successfully")


# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
@app.on_event("startup")
async def startup_event():
    stock_scheduler.start()
    logger.info("Stock scheduler started on application startup")

    # DB ë§ˆì´ê·¸ë ˆì´ì…˜ (ëˆ„ë½ ì»¬ëŸ¼ ì¶”ê°€)
    db = next(get_db())
    try:
        from sqlalchemy import text
        db.execute(text('ALTER TABLE stocks ADD COLUMN IF NOT EXISTS history_updated_at TIMESTAMP'))
        db.execute(text('ALTER TABLE stocks ADD COLUMN IF NOT EXISTS signal_analyzed_at TIMESTAMP'))
        db.commit()
        logger.info("DB migration completed")
    except Exception as e:
        logger.warning(f"DB migration skipped: {e}")
        db.rollback()

    # ê¸°ë³¸ íƒœê·¸ ìƒì„±
    try:
        seed_default_tags(db)
    finally:
        db.close()

@app.on_event("shutdown")
async def shutdown_event():
    stock_scheduler.stop()
    logger.info("Stock scheduler stopped on application shutdown")

@app.get("/")
def read_root():
    return {"message": "Stock Analyzer API", "version": "1.0.0"}

@app.get("/api/stocks", response_model=schemas.StockListResponse)
def get_stocks(
    market: Optional[str] = Query(None, description="Filter by market (KR, US)"),
    exchange: Optional[str] = Query(None, description="Filter by exchange"),
    sector: Optional[str] = Query(None, description="Filter by sector"),
    exclude_etf: bool = Query(False, description="Exclude ETF and index funds"),
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    order_by: Optional[str] = Query("market_cap", description="Sort field (market_cap, change_percent)"),
    order_dir: Optional[str] = Query("desc", description="Sort direction (asc, desc)"),
    nocache: bool = Query(False, description="Skip cache and fetch fresh data"),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    # ìºì‹œ í‚¤ ìƒì„± (ìœ ì €ë³„, ì¡°ê±´ë³„ë¡œ êµ¬ë¶„)
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "user": user_token,
        "market": market,
        "exchange": exchange,
        "sector": sector,
        "exclude_etf": exclude_etf,
        "skip": skip,
        "limit": limit,
        "order_by": order_by,
        "order_dir": order_dir
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸ (nocache=trueë©´ ìŠ¤í‚µ)
    if not nocache:
        cached_data = get_cache(cache_key)
        if cached_data:
            logger.info(f"âœ… Cache HIT for {user_token[:8]}... {market=} {skip=}")
            return cached_data

    logger.info(f"â³ Cache MISS for {user_token[:8]}... {market=} {skip=}")

    query = db.query(Stock).filter(Stock.is_active == True)

    # 'ì œì™¸', 'ì—ëŸ¬', 'ì‚­ì œ' íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª© ì œì™¸ (ì‚¬ìš©ìë³„)
    if current_user:
        exclude_tags = db.query(StockTag).filter(
            StockTag.name.in_(["dislike", "error", "delete"])
        ).all()

        if exclude_tags:
            exclude_tag_ids = [tag.id for tag in exclude_tags]
            exclude_stock_ids = db.query(StockTagAssignment.stock_id).filter(
                StockTagAssignment.tag_id.in_(exclude_tag_ids),
                StockTagAssignment.user_token == current_user.user_token
            ).all()
            exclude_stock_ids = [sid[0] for sid in exclude_stock_ids]
            if exclude_stock_ids:
                query = query.filter(~Stock.id.in_(exclude_stock_ids))

    # ETF ë° ì§€ìˆ˜ ì¢…ëª© ì œì™¸
    if exclude_etf:
        for keyword in ETF_KEYWORDS:
            query = query.filter(~Stock.name.ilike(f'%{keyword}%'))

    if market:
        query = query.filter(Stock.market == market)
    if exchange:
        query = query.filter(Stock.exchange == exchange)
    if sector:
        query = query.filter(Stock.sector == sector)

    # ë™ì  ì •ë ¬: order_by, order_dir íŒŒë¼ë¯¸í„° ê¸°ë°˜
    # ë™ì¼ ê°’ì¼ ë•Œ ì¼ê´€ëœ ì •ë ¬ì„ ìœ„í•´ ë³´ì¡° í‚¤ ì¶”ê°€
    if order_by == "change_percent":
        if order_dir == "asc":
            query = query.order_by(
                Stock.change_percent.asc().nullslast(),
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )
        else:
            query = query.order_by(
                Stock.change_percent.desc().nullslast(),
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )
    else:
        # ê¸°ë³¸: ì‹œê°€ì´ì•¡ìˆœ
        if order_dir == "asc":
            query = query.order_by(
                Stock.market_cap.asc().nullslast(),
                Stock.id.asc()
            )
        else:
            query = query.order_by(
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )

    # COUNT ìµœì í™”: ì²« í˜ì´ì§€(skip==0)ì—ì„œë§Œ ì •í™•í•œ count ê³„ì‚°
    # ì´í›„ í˜ì´ì§€ì—ì„œëŠ” ìºì‹œëœ ê°’ ì‚¬ìš© (3-5ë°° ì†ë„ í–¥ìƒ)
    if skip == 0:
        total = query.count()
    else:
        # ì´ì „ í˜ì´ì§€ì—ì„œ ìºì‹œëœ total ì‚¬ìš© (ì¶”ì •ì¹˜)
        # ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ count ê³„ì‚°
        count_cache_key = f"count:{hashlib.md5(orjson.dumps({**cache_key_data, 'skip': 0}, option=orjson.OPT_SORT_KEYS)).hexdigest()}"
        cached_first_page = get_cache(count_cache_key)
        if cached_first_page and 'total' in cached_first_page:
            total = cached_first_page['total']
        else:
            total = query.count()

    stocks = query.offset(skip).limit(limit).all()

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„) - íƒœê·¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ
    tags_map = {}
    tags_by_id = {}
    latest_tag_dates = {}
    if current_user:
        from sqlalchemy import and_

        # ëª¨ë“  ì£¼ì‹ì˜ íƒœê·¸ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            and_(
                StockTagAssignment.stock_id.in_(stock_ids),
                StockTagAssignment.user_token == current_user.user_token
            )
        ).order_by(StockTagAssignment.created_at.desc()).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
                latest_tag_dates[ta.stock_id] = ta.created_at
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ ë°˜í™˜
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨ (ê°„ë‹¨ ê³„ì‚°)
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡ (ì´ë¯¸ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©) - ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        tags = []
        latest_tag_date = None
        if current_user and stock.id in tags_map:
            latest_tag_date = latest_tag_dates.get(stock.id)
            for ta in tags_map[stock.id]:
                tag_obj = tags_by_id.get(ta.tag_id)
                if tag_obj:
                    tags.append({
                        "id": tag_obj.id,
                        "name": tag_obj.name,
                        "display_name": tag_obj.display_name,
                        "color": tag_obj.color,
                        "icon": tag_obj.icon,
                        "order": tag_obj.order,
                    })

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": latest_tag_date,

            # íˆìŠ¤í† ë¦¬ ë°ì´í„° ìƒíƒœ
            "history_records_count": stock.history_records_count or 0,
            "has_history_data": (stock.history_records_count or 0) > 0,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ í•„ë“œë§Œ ìœ ì§€
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥
    result = {
        "total": total,
        "stocks": stock_list,
        "page": skip // limit + 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=300)  # 5ë¶„ ìºì‹œ
    return result

@app.get("/api/stocks/search", response_model=schemas.StockListResponse)
def search_stocks(
    q: str = Query(..., min_length=1, description="Search query (name or symbol)"),
    market: Optional[str] = Query(None, description="Filter by market (KR, US)"),
    limit: int = Query(10, ge=1, le=50),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """ì¢…ëª© ê²€ìƒ‰ API - ì¢…ëª©ëª… ë˜ëŠ” ì‹¬ë³¼ë¡œ ê²€ìƒ‰ (ìë™ì™„ì„±ìš©)"""
    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "search",
        "user": user_token,
        "q": q.lower(),
        "market": market,
        "limit": limit
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Search cache HIT for '{q}'")
        return cached_data

    logger.info(f"â³ Search cache MISS for '{q}'")

    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    query = db.query(Stock).filter(Stock.is_active == True)

    # 'ì œì™¸', 'ì—ëŸ¬', 'ì‚­ì œ' íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª© ì œì™¸ (ì‚¬ìš©ìë³„)
    if current_user:
        exclude_tags = db.query(StockTag).filter(
            StockTag.name.in_(["dislike", "error", "delete"])
        ).all()

        if exclude_tags:
            exclude_tag_ids = [tag.id for tag in exclude_tags]
            exclude_stock_ids = db.query(StockTagAssignment.stock_id).filter(
                StockTagAssignment.tag_id.in_(exclude_tag_ids),
                StockTagAssignment.user_token == current_user.user_token
            ).all()
            exclude_stock_ids = [sid[0] for sid in exclude_stock_ids]
            if exclude_stock_ids:
                query = query.filter(~Stock.id.in_(exclude_stock_ids))

    # ì¢…ëª©ëª… ë˜ëŠ” ì‹¬ë³¼ë¡œ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    search_filter = (
        Stock.name.ilike(f'%{q}%') |
        Stock.symbol.ilike(f'%{q}%')
    )
    query = query.filter(search_filter)

    # ë§ˆì¼“ í•„í„°
    if market:
        query = query.filter(Stock.market == market)

    # ì‹œê°€ì´ì•¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê²€ìƒ‰ ê²°ê³¼ì—ì„œë„ í° ê¸°ì—…ì´ ë¨¼ì €)
    query = query.order_by(
        Stock.market_cap.desc().nullslast(),
        Stock.id.asc()
    )

    # ì œí•œëœ ìˆ˜ë§Œ ê°€ì ¸ì˜¤ê¸° (ìë™ì™„ì„±ìš©)
    stocks = query.limit(limit).all()
    total = len(stocks)

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„)
    tags_map = {}
    tags_by_id = {}
    latest_tag_dates = {}
    if current_user and stocks:
        from sqlalchemy import and_

        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            and_(
                StockTagAssignment.stock_id.in_(stock_ids),
                StockTagAssignment.user_token == current_user.user_token
            )
        ).order_by(StockTagAssignment.created_at.desc()).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
                latest_tag_dates[ta.stock_id] = ta.created_at
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡ - ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        tags = []
        latest_tag_date = None
        if current_user and stock.id in tags_map:
            latest_tag_date = latest_tag_dates.get(stock.id)
            for ta in tags_map[stock.id]:
                tag_obj = tags_by_id.get(ta.tag_id)
                if tag_obj:
                    tags.append({
                        "id": tag_obj.id,
                        "name": tag_obj.name,
                        "display_name": tag_obj.display_name,
                        "color": tag_obj.color,
                        "icon": tag_obj.icon,
                        "order": tag_obj.order,
                    })

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": latest_tag_date,

            # íˆìŠ¤í† ë¦¬ ë°ì´í„° ìƒíƒœ
            "history_records_count": stock.history_records_count or 0,
            "has_history_data": (stock.history_records_count or 0) > 0,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ í•„ë“œ
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥ (ê²€ìƒ‰ì€ 1ë¶„ ìºì‹œ)
    result = {
        "total": total,
        "stocks": stock_list,
        "page": 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=60)  # 1ë¶„ ìºì‹œ
    return result


@app.get("/api/stocks/{stock_id}", response_model=schemas.Stock)
def get_stock(stock_id: int, db: Session = Depends(get_db)):
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")
    return stock

@app.get("/api/stocks/{stock_id}/prices", response_model=List[schemas.StockPrice])
def get_stock_prices(
    stock_id: int,
    start_date: Optional[date] = Query(None),
    end_date: Optional[date] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(StockPrice).filter(StockPrice.stock_id == stock_id)

    if start_date:
        query = query.filter(StockPrice.date >= start_date)
    if end_date:
        query = query.filter(StockPrice.date <= end_date)

    prices = query.order_by(StockPrice.date.desc()).limit(500).all()
    return prices

@app.get("/api/stocks/{stock_id}/daily-data", response_model=List[schemas.StockDailyData])
def get_stock_daily_data(
    stock_id: int,
    start_date: Optional[date] = Query(None),
    end_date: Optional[date] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(StockDailyData).filter(StockDailyData.stock_id == stock_id)

    if start_date:
        query = query.filter(StockDailyData.date >= start_date)
    if end_date:
        query = query.filter(StockDailyData.date <= end_date)

    daily_data = query.order_by(StockDailyData.date.desc()).limit(500).all()
    return daily_data

def run_background_crawl(market: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  í¬ë¡¤ë§ ì‘ì—…"""
    try:
        logger.info(f"Starting background crawl for market: {market}")
        result = crawler_manager.update_stock_list(market)

        # ETF í•„í„°ë§ ì •ë³´ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±
        etf_info = ""
        if result.get('skipped_etf', 0) > 0:
            etf_info = f" ({result['skipped_etf']} ETF/Index stocks filtered out)"

        logger.info(f"Background crawl completed: {result['success']} out of {result['total']} stocks{etf_info}")

        # ìºì‹œ ë¬´íš¨í™”
        if redis_client:
            try:
                # ëª¨ë“  stocks ê´€ë ¨ ìºì‹œ ì‚­ì œ
                for key in redis_client.scan_iter("cache:*stocks*"):
                    redis_client.delete(key)
                logger.info("Cache invalidated after crawling")
            except Exception as e:
                logger.error(f"Failed to invalidate cache: {e}")

    except Exception as e:
        logger.error(f"Error during background stock list crawling: {str(e)}")


@app.post("/api/crawl/stocks", response_model=schemas.CrawlingStatus)
def crawl_stock_list(
    background_tasks: BackgroundTasks,
    market: str = Query("ALL", pattern="^(ALL|KR|US)$"),
    current_user: User = Depends(get_current_user)
):
    """ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§ - 10ë¶„ ì¿¨íƒ€ì„ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)"""
    global last_crawl_time

    # ì¿¨íƒ€ì„ ì²´í¬
    if last_crawl_time:
        elapsed_time = datetime.utcnow() - last_crawl_time
        remaining_seconds = (CRAWL_COOLDOWN_MINUTES * 60) - elapsed_time.total_seconds()

        if remaining_seconds > 0:
            remaining_minutes = int(remaining_seconds // 60)
            remaining_secs = int(remaining_seconds % 60)
            raise HTTPException(
                status_code=429,
                detail=f"í¬ë¡¤ë§ ì¿¨íƒ€ì„ì…ë‹ˆë‹¤. {remaining_minutes}ë¶„ {remaining_secs}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )

    # ì¿¨íƒ€ì„ ì—…ë°ì´íŠ¸
    last_crawl_time = datetime.utcnow()

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€
    background_tasks.add_task(run_background_crawl, market)

    # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    return {
        "success": 0,
        "failed": 0,
        "total": 0,
        "skipped_etf": 0,
        "message": f"í¬ë¡¤ë§ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œê¹Œì§€ ì•½ 20ì´ˆ ì†Œìš”ë©ë‹ˆë‹¤."
    }


@app.post("/api/crawl/indicators/{stock_id}")
def calculate_indicators(stock_id: int, db: Session = Depends(get_db)):
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    try:
        crawler_manager.calculate_technical_indicators(stock_id)
        return {"message": f"Successfully calculated indicators for {stock.symbol}"}
    except Exception as e:
        logger.error(f"Error calculating indicators: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/scheduler/status")
def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë° ë“±ë¡ëœ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        jobs = stock_scheduler.get_jobs()
        return {
            "running": stock_scheduler.scheduler.running,
            "jobs": jobs,
            "message": "Scheduler is running" if stock_scheduler.scheduler.running else "Scheduler is stopped"
        }
    except Exception as e:
        logger.error(f"Error getting scheduler status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/scheduler/trigger")
def trigger_manual_crawl():
    """ìˆ˜ë™ìœ¼ë¡œ ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        result = stock_scheduler.trigger_manual_crawl()
        return {
            "message": "Manual crawling completed successfully",
            "result": result
        }
    except Exception as e:
        logger.error(f"Error in manual crawling: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/scheduler/trigger-history")
def trigger_manual_history_collection(
    background_tasks: BackgroundTasks,
    days: int = Query(100, ge=1, le=365, description="Number of days to collect")
):
    """ìˆ˜ë™ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    def run_collection():
        try:
            logger.info(f"ğŸš€ Background history collection started ({days} days)...")
            result = stock_scheduler.trigger_manual_history_collection(days=days)
            logger.info(f"âœ… Background history collection completed: {result}")
        except Exception as e:
            logger.error(f"âŒ Error in background history collection: {str(e)}")

    background_tasks.add_task(run_collection)

    return {
        "success": True,
        "message": f"íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ ({days}ì¼ì¹˜ ë°ì´í„°)",
        "days": days,
        "mode": settings.HISTORY_COLLECTION_MODE,
        "note": "Check Railway logs for progress"
    }

@app.get("/api/stocks/{stock_id}/price-history", response_model=List[schemas.StockPriceHistory])
def get_stock_price_history(
    stock_id: int,
    days: int = Query(30, ge=1, le=365, description="Number of days to retrieve"),
    db: Session = Depends(get_db)
):
    """íŠ¹ì • ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ìµœê·¼ Nì¼ ë°ì´í„° ì¡°íšŒ
    from datetime import date, timedelta
    start_date = date.today() - timedelta(days=days)

    price_history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date
    ).order_by(StockPriceHistory.date.desc()).all()

    return price_history

@app.post("/api/stocks/{stock_id}/crawl-history")
def crawl_stock_price_history(
    stock_id: int,
    days: int = Query(100, ge=1, le=365, description="Number of days to crawl"),
    db: Session = Depends(get_db)
):
    """ê°œë³„ ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ í¬ë¡¤ë§"""
    try:
        # ì¢…ëª© ì¡´ì¬ í™•ì¸
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        logger.info(f"Starting price history crawling for stock {stock.symbol}")

        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        latest_record = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).order_by(StockPriceHistory.date.desc()).first()

        # í¬ë¡¤ë§ ì‹¤í–‰
        price_data = price_history_crawler.fetch_price_history(stock.symbol, days)

        if not price_data:
            return {
                "success": 0,
                "failed": 1,
                "total": 1,
                "message": f"No price data found for {stock.symbol}"
            }

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        success_count = 0
        failed_count = 0

        for data in price_data:
            try:
                # ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                existing = db.query(StockPriceHistory).filter(
                    StockPriceHistory.stock_id == stock_id,
                    StockPriceHistory.date == data['date']
                ).first()

                if existing:
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    existing.open_price = data['open_price']
                    existing.high_price = data['high_price']
                    existing.low_price = data['low_price']
                    existing.close_price = data['close_price']
                    existing.volume = data['volume']
                    existing.updated_at = datetime.utcnow()
                else:
                    # ìƒˆ ë°ì´í„° ì¶”ê°€
                    new_record = StockPriceHistory(
                        stock_id=stock_id,
                        **data
                    )
                    db.add(new_record)

                success_count += 1

            except Exception as e:
                logger.error(f"Error saving price data for {data['date']}: {str(e)}")
                failed_count += 1
                continue

        # ì»¤ë°‹
        db.commit()

        logger.info(f"Price history crawling completed for {stock.symbol}: {success_count} success, {failed_count} failed")

        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(price_data),
            "message": f"Successfully crawled {success_count} price records for {stock.symbol}"
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error crawling price history for stock {stock_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stocks/{stock_id}/history-status")
def get_stock_history_status(stock_id: int, db: Session = Depends(get_db)):
    """ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ìƒíƒœ í™•ì¸"""
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # íˆìŠ¤í† ë¦¬ ë°ì´í„° í†µê³„
    total_records = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).count()

    latest_record = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).order_by(StockPriceHistory.date.desc()).first()

    oldest_record = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).order_by(StockPriceHistory.date.asc()).first()

    return {
        "stock_symbol": stock.symbol,
        "stock_name": stock.name,
        "total_records": total_records,
        "latest_date": latest_record.date if latest_record else None,
        "oldest_date": oldest_record.date if oldest_record else None,
        "has_data": total_records > 0
    }

@app.post("/api/stocks/{stock_id}/sync-history")
def sync_stock_history(
    stock_id: int,
    days: int = Query(100, ge=1, le=365, description="ìˆ˜ì§‘í•  ì¼ìˆ˜ (ì „ì²´ ìˆ˜ì§‘ ì‹œ)"),
    db: Session = Depends(get_db)
):
    """
    ê°œë³„ ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ë™ê¸°í™” (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)

    - ë°ì´í„° ì—†ìŒ/ë¶€ì¡±: ì „ì²´ ìˆ˜ì§‘ (full)
    - ìµœì‹  ë°ì´í„° ìˆìŒ: ìŠ¤í‚µ (skip)
    - ë©°ì¹  ë¹ ì§: ì¦ë¶„ ìˆ˜ì§‘ (incremental)
    """
    try:
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        # ì‹œì¥ë³„ ì²˜ë¦¬
        if stock.market == 'KR':
            # í•œêµ­ ì£¼ì‹: KIS API ì‚¬ìš©
            from app.crawlers.kis_history_crawler import kis_history_crawler

            # ìˆ˜ì§‘ í•„ìš” ì—¬ë¶€ í™•ì¸ (í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)
            should_collect, mode, last_date = kis_history_crawler._should_collect_history(stock, db)

            if mode == "skip":
                return {
                    "success": True,
                    "mode": "skip",
                    "message": f"ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤ (ë§ˆì§€ë§‰: {last_date})",
                    "stock_id": stock_id,
                    "symbol": stock.symbol,
                    "name": stock.name,
                    "records_count": stock.history_records_count,
                    "last_date": str(last_date) if last_date else None,
                    "records_added": 0
                }

            # ìˆ˜ì§‘ ì‹¤í–‰
            if mode == "incremental":
                incremental_start = last_date + timedelta(days=1)
                result = kis_history_crawler.collect_history_for_stock(stock, start_date=incremental_start, db=db)
                logger.info(f"Incremental sync for {stock.symbol} from {incremental_start}")
            else:
                result = kis_history_crawler.collect_history_for_stock(stock, days=days, db=db)
                logger.info(f"Full sync for {stock.symbol} ({days} days)")

        elif stock.market == 'US':
            # ë¯¸êµ­ ì£¼ì‹: KIS API ì‚¬ìš©
            from app.kis.kis_client import get_kis_client

            kis_client = get_kis_client()
            if not kis_client:
                raise HTTPException(status_code=500, detail="KIS APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

            # ê±°ë˜ì†Œ ì½”ë“œ ë³€í™˜ (NASDAQ -> NAS, NYSE -> NYS)
            exchange_map = {
                'NASDAQ': 'NAS',
                'NYSE': 'NYS',
                'AMEX': 'AMS',
                'NAS': 'NAS',
                'NYS': 'NYS',
            }
            kis_exchange = exchange_map.get(stock.exchange, 'NAS')

            logger.info(f"Fetching US stock history for {stock.symbol} ({kis_exchange}) via KIS API")

            # KIS APIë¡œ OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            ohlcv_data = kis_client.get_us_stock_ohlcv(stock.symbol, exchange=kis_exchange)

            if not ohlcv_data:
                return {
                    "success": False,
                    "mode": "full",
                    "message": "KIS APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "stock_id": stock_id,
                    "symbol": stock.symbol,
                    "name": stock.name,
                    "records_count": stock.history_records_count or 0,
                    "last_date": None,
                    "records_added": 0
                }

            # ê°€ê²© ë°ì´í„° ì €ì¥
            records_added = 0
            for item in ohlcv_data:
                try:
                    # KIS API ì‘ë‹µ í•„ë“œ: xymd(ë‚ ì§œ), open, high, low, clos, tvol
                    date_str = item.get('xymd', '')
                    if not date_str:
                        continue

                    price_date = datetime.strptime(date_str, '%Y%m%d').date()

                    # ì¤‘ë³µ ì²´í¬
                    existing = db.query(StockPriceHistory).filter(
                        StockPriceHistory.stock_id == stock_id,
                        StockPriceHistory.date == price_date
                    ).first()

                    if not existing:
                        history_record = StockPriceHistory(
                            stock_id=stock_id,
                            date=price_date,
                            open_price=float(item.get('open', 0)),
                            high_price=float(item.get('high', 0)),
                            low_price=float(item.get('low', 0)),
                            close_price=float(item.get('clos', 0)),
                            volume=int(item.get('tvol', 0))
                        )
                        db.add(history_record)
                        records_added += 1
                except Exception as e:
                    logger.error(f"Error saving US price data: {e}")
                    continue

            db.commit()
            mode = "full"
            result = {"success": True, "records_saved": records_added}
            logger.info(f"US stock sync for {stock.symbol}: {records_added} records added")

        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œì¥ì…ë‹ˆë‹¤: {stock.market}")

        # ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
        updated_count = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).count()

        # Stockì˜ history_records_count ì—…ë°ì´íŠ¸
        db.execute(
            text("UPDATE stocks SET history_records_count = :count WHERE id = :id"),
            {"count": updated_count, "id": stock_id}
        )
        db.commit()

        # ìµœì‹  ë‚ ì§œ ì¡°íšŒ
        latest = db.query(StockPriceHistory.date).filter(
            StockPriceHistory.stock_id == stock_id
        ).order_by(StockPriceHistory.date.desc()).first()

        return {
            "success": result.get("success", False),
            "mode": mode,
            "message": f"{mode} ìˆ˜ì§‘ ì™„ë£Œ",
            "stock_id": stock_id,
            "symbol": stock.symbol,
            "name": stock.name,
            "records_count": updated_count,
            "last_date": str(latest[0]) if latest else None,
            "records_added": result.get("records_saved", 0)
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error syncing history for stock {stock_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/stocks/cleanup-etf")
def cleanup_etf_stocks(db: Session = Depends(get_db)):
    """ì§€ìˆ˜/ETF ì¢…ëª©ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì™„ì „íˆ ì‚­ì œ"""
    try:
        # ì‚­ì œí•  ì¢…ëª©ë“¤ ì°¾ê¸°
        etf_stocks = []
        for keyword in ETF_KEYWORDS:
            stocks = db.query(Stock).filter(Stock.name.ilike(f'%{keyword}%')).all()
            etf_stocks.extend(stocks)

        # ì¤‘ë³µ ì œê±°
        unique_etf_stocks = list({stock.id: stock for stock in etf_stocks}.values())

        logger.info(f"Found {len(unique_etf_stocks)} ETF/Index stocks to delete")

        deleted_count = 0
        deleted_stocks = []

        for stock in unique_etf_stocks:
            try:
                # ê´€ë ¨ ë°ì´í„°ë„ í•¨ê»˜ ì‚­ì œ (cascadeë¡œ ìë™ ì‚­ì œë¨)
                deleted_stocks.append({
                    "symbol": stock.symbol,
                    "name": stock.name
                })

                db.delete(stock)
                deleted_count += 1

            except Exception as e:
                logger.error(f"Error deleting stock {stock.symbol}: {str(e)}")
                continue

        # ì»¤ë°‹
        db.commit()

        logger.info(f"Successfully deleted {deleted_count} ETF/Index stocks")

        return {
            "deleted_count": deleted_count,
            "deleted_stocks": deleted_stocks,
            "message": f"Successfully deleted {deleted_count} ETF/Index stocks from database"
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error during ETF cleanup: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/stocks/{stock_id}")
def delete_stock(stock_id: int, db: Session = Depends(get_db)):
    """
    ì¢…ëª©ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ë¥¼ ì™„ì „ ì‚­ì œ
    """
    try:
        # ì¢…ëª© ì¡´ì¬ í™•ì¸
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        # í•´ë‹¹ ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì‚­ì œ
        history_count = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).count()

        db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).delete()

        # ì¢…ëª© ë°ì´í„° ì‚­ì œ
        db.delete(stock)
        db.commit()

        logger.info(f"Deleted stock {stock.symbol} ({stock.name}) and {history_count} history records")

        return {
            "success": True,
            "message": f"ì¢…ëª© '{stock.name}({stock.symbol})'ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_history_count": history_count
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error deleting stock {stock_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete stock: {str(e)}")

@app.post("/api/stocks/{stock_id}/analyze")
async def analyze_single_stock(
    stock_id: int,
    db: Session = Depends(get_db)
):
    """
    ë‹¨ì¼ ì¢…ëª© ë¶„ì„: ë„¤ì´ë²„ì—ì„œ ìƒì„¸ ì •ë³´ ë° ì¼ë³„ ê°€ê²© í¬ë¡¤ë§
    ì¤‘ë³µ ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
    """
    try:
        # ì¢…ëª© ì¡°íšŒ
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        logger.info(f"Analyzing stock: {stock.symbol} ({stock.name})")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        from app.crawlers.naver_us_crawler import NaverUSStockCrawler
        crawler = NaverUSStockCrawler()

        # ì¢…ëª© ë¶„ì„ ì‹¤í–‰
        # US ì£¼ì‹ì€ sector í•„ë“œì— reuters_code (ì˜ˆ: NVDA.O)ê°€ ì €ì¥ë˜ì–´ ìˆìŒ
        symbol_to_use = stock.sector if stock.market == "US" and stock.sector else stock.symbol
        result = crawler.analyze_single_stock(symbol_to_use)

        if not result['success']:
            raise HTTPException(status_code=500, detail=result['message'])

        stats = {
            'new_records': 0,
            'duplicate_records': 0,
            'updated_overview': False
        }

        # Overview ì •ë³´ ì—…ë°ì´íŠ¸
        if result['overview']:
            overview = result['overview']
            stock.current_price = overview.get('current_price', stock.current_price)
            stock.change_amount = overview.get('change_amount', stock.change_amount)
            stock.change_percent = overview.get('change_percent', stock.change_percent)
            stock.previous_close = overview.get('previous_close', stock.previous_close)
            stock.market_cap = overview.get('market_cap', stock.market_cap)
            stock.trading_volume = overview.get('volume', stock.trading_volume)
            stock.updated_at = datetime.utcnow()
            stats['updated_overview'] = True
            logger.info(f"Updated overview for {stock.symbol}")

        # ê°€ê²© íˆìŠ¤í† ë¦¬ ì €ì¥ (ì¤‘ë³µ ì²´í¬)
        if result['price_history']:
            for price_data in result['price_history']:
                try:
                    price_date = datetime.strptime(price_data['date'], '%Y%m%d').date()

                    # ì¤‘ë³µ ì²´í¬
                    existing_record = db.query(StockPriceHistory).filter(
                        StockPriceHistory.stock_id == stock.id,
                        StockPriceHistory.date == price_date
                    ).first()

                    if existing_record:
                        stats['duplicate_records'] += 1
                        continue

                    # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                    price_history = StockPriceHistory(
                        stock_id=stock.id,
                        date=price_date,
                        open_price=price_data['open_price'],
                        high_price=price_data['high_price'],
                        low_price=price_data['low_price'],
                        close_price=price_data['close_price'],
                        volume=price_data['volume']
                    )
                    db.add(price_history)
                    stats['new_records'] += 1

                except Exception as e:
                    logger.error(f"Error saving price record: {e}")
                    continue

        db.commit()

        # ìµœì‹  ê°±ì‹  ë‚ ì§œ ì¡°íšŒ
        latest_record = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock.id
        ).order_by(StockPriceHistory.date.desc()).first()

        return {
            "success": True,
            "stock_id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "stats": stats,
            "latest_update_date": latest_record.date.isoformat() if latest_record else None,
            "total_records": db.query(StockPriceHistory).filter(
                StockPriceHistory.stock_id == stock.id
            ).count(),
            "message": f"Successfully analyzed {stock.symbol}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing stock {stock_id}: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to analyze stock: {str(e)}")

# ===== íƒœê·¸ ê´€ë¦¬ API =====

@app.get("/api/tags", response_model=schemas.TagListResponse)
def get_tags(db: Session = Depends(get_db)):
    """ëª¨ë“  íƒœê·¸ ëª©ë¡ ì¡°íšŒ"""
    tags = db.query(StockTag).filter(StockTag.is_active == True).order_by(StockTag.order).all()
    return {"tags": tags}

@app.post("/api/tags", response_model=schemas.StockTag)
def create_tag(
    tag: schemas.StockTagCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ìƒˆ íƒœê·¸ ìƒì„± - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create tags")

    # ì¤‘ë³µ ì²´í¬
    existing_tag = db.query(StockTag).filter(StockTag.name == tag.name).first()
    if existing_tag:
        raise HTTPException(status_code=400, detail="Tag with this name already exists")

    new_tag = StockTag(**tag.dict())
    db.add(new_tag)
    db.commit()
    db.refresh(new_tag)
    return new_tag

@app.put("/api/tags/{tag_id}", response_model=schemas.StockTag)
def update_tag(
    tag_id: int,
    tag: schemas.StockTagCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íƒœê·¸ ì—…ë°ì´íŠ¸ - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can update tags")

    existing_tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not existing_tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì´ë¦„ ë³€ê²½ ì‹œ ì¤‘ë³µ ì²´í¬
    if tag.name != existing_tag.name:
        duplicate = db.query(StockTag).filter(StockTag.name == tag.name).first()
        if duplicate:
            raise HTTPException(status_code=400, detail="Tag with this name already exists")

    # ì—…ë°ì´íŠ¸
    for key, value in tag.dict().items():
        setattr(existing_tag, key, value)

    db.commit()
    db.refresh(existing_tag)
    return existing_tag

@app.delete("/api/tags/{tag_id}")
def delete_tag(
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íƒœê·¸ ì‚­ì œ - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can delete tags")

    tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # í• ë‹¹ëœ íƒœê·¸ë„ ëª¨ë‘ ì‚­ì œ (cascadeë¡œ ìë™ ì²˜ë¦¬ë¨)
    db.delete(tag)
    db.commit()

    return {"success": True, "message": f"Tag '{tag.display_name}' deleted successfully"}

@app.post("/api/stocks/{stock_id}/tags/{tag_id}", response_model=schemas.TagAssignmentResponse)
def add_tag_to_stock(
    stock_id: int,
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì¢…ëª©ì— íƒœê·¸ ì¶”ê°€ (ì‚¬ìš©ìë³„)"""
    # ì¢…ëª© ì¡´ì¬ í™•ì¸
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # íƒœê·¸ ì¡´ì¬ í™•ì¸
    tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì´ë¯¸ í• ë‹¹ëœ íƒœê·¸ì¸ì§€ í™•ì¸ (ì‚¬ìš©ìë³„)
    existing = db.query(StockTagAssignment).filter(
        StockTagAssignment.stock_id == stock_id,
        StockTagAssignment.tag_id == tag_id,
        StockTagAssignment.user_token == current_user.user_token
    ).first()

    if existing:
        return {"message": "Tag already assigned to this stock", "tag": tag}

    # ìƒˆ í• ë‹¹ ìƒì„± (ì‚¬ìš©ì í† í° í¬í•¨)
    assignment = StockTagAssignment(stock_id=stock_id, tag_id=tag_id, user_token=current_user.user_token)
    db.add(assignment)
    db.commit()

    # ìºì‹œ ë¬´íš¨í™”
    invalidate_cache()

    return {"message": f"Tag '{tag.display_name}' added to {stock.name}", "tag": tag}

@app.delete("/api/stocks/{stock_id}/tags/{tag_id}")
def remove_tag_from_stock(
    stock_id: int,
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì¢…ëª©ì—ì„œ íƒœê·¸ ì œê±° (ì‚¬ìš©ìë³„)"""
    assignment = db.query(StockTagAssignment).filter(
        StockTagAssignment.stock_id == stock_id,
        StockTagAssignment.tag_id == tag_id,
        StockTagAssignment.user_token == current_user.user_token
    ).first()

    if not assignment:
        raise HTTPException(status_code=404, detail="Tag assignment not found")

    db.delete(assignment)
    db.commit()

    # ìºì‹œ ë¬´íš¨í™”
    invalidate_cache()

    return {"message": "Tag removed from stock"}

@app.get("/api/stocks/by-tag/{tag_name}", response_model=schemas.StockListResponse)
def get_stocks_by_tag(
    tag_name: str,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """íŠ¹ì • íƒœê·¸ê°€ ë¶€ì—¬ëœ ì¢…ëª© ëª©ë¡ ì¡°íšŒ (ì‚¬ìš©ìë³„) - ìµœì í™”ë¨"""

    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "by-tag",
        "user": user_token,
        "tag_name": tag_name,
        "skip": skip,
        "limit": limit
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Cache HIT for tag {tag_name}, user {user_token[:8]}...")
        return cached_data

    logger.info(f"â³ Cache MISS for tag {tag_name}, user {user_token[:8]}...")

    # ì¸ì¦ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ê²°ê³¼
    if not current_user:
        result = {"total": 0, "stocks": [], "page": 1, "page_size": limit}
        set_cache(cache_key, result, ttl=300)
        return result

    # íƒœê·¸ ì°¾ê¸°
    tag = db.query(StockTag).filter(StockTag.name == tag_name).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì¢…ëª© ì¡°íšŒ (JOINìœ¼ë¡œ í•œ ë²ˆì—)
    query = db.query(Stock).join(
        StockTagAssignment,
        (StockTagAssignment.stock_id == Stock.id) &
        (StockTagAssignment.tag_id == tag.id) &
        (StockTagAssignment.user_token == current_user.user_token)
    ).filter(Stock.is_active == True)

    # ì¼ê´€ëœ ì •ë ¬: ì‹œê°€ì´ì•¡ ë‚´ë¦¼ì°¨ìˆœ
    query = query.order_by(
        Stock.market_cap.desc().nullslast(),
        Stock.id.asc()
    )

    # COUNT ìµœì í™”: ì²« í˜ì´ì§€ì—ì„œë§Œ ì •í™•í•œ count ê³„ì‚°
    if skip == 0:
        total = query.count()
    else:
        # ì´ì „ í˜ì´ì§€ì—ì„œ ìºì‹œëœ total ì‚¬ìš©
        count_cache_key = f"count:{hashlib.md5(orjson.dumps({**cache_key_data, 'skip': 0}, option=orjson.OPT_SORT_KEYS)).hexdigest()}"
        cached_first_page = get_cache(count_cache_key)
        if cached_first_page and 'total' in cached_first_page:
            total = cached_first_page['total']
        else:
            total = query.count()

    stocks = query.offset(skip).limit(limit).all()

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    tags_map = {}
    tags_by_id = {}
    if stocks:
        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            StockTagAssignment.stock_id.in_(stock_ids),
            StockTagAssignment.user_token == current_user.user_token
        ).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ ë°˜í™˜
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡ (ì´ë¯¸ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©) - ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        tags = []
        if stock.id in tags_map:
            for ta in tags_map[stock.id]:
                tag_obj = tags_by_id.get(ta.tag_id)
                if tag_obj:
                    tags.append({
                        "id": tag_obj.id,
                        "name": tag_obj.name,
                        "display_name": tag_obj.display_name,
                        "color": tag_obj.color,
                        "icon": tag_obj.icon,
                        "order": tag_obj.order,
                    })

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": None,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œë“¤
            "face_value": stock.face_value,
            "shares_outstanding": stock.shares_outstanding,
            "foreign_ratio": stock.foreign_ratio,
            "history_records_count": stock.history_records_count or 0,
            "history_latest_date": None,
            "history_oldest_date": None,
            "has_history_data": (stock.history_records_count or 0) > 0,
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥
    result = {
        "total": total,
        "stocks": stock_list,
        "page": skip // limit + 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=300)  # 5ë¶„ ìºì‹œ
    return result

# ===== Authentication APIs =====

@app.post("/api/auth/register", response_model=schemas.TokenResponse)
def register(
    user_data: schemas.UserRegister,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íšŒì›ê°€ì… - ê´€ë¦¬ìë§Œ ìƒˆ ì‚¬ìš©ì ìƒì„± ê°€ëŠ¥"""
    # ê´€ë¦¬ì ê¶Œí•œ í™•ì¸
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create new users")

    # ë‹‰ë„¤ì„ ì¤‘ë³µ ì²´í¬
    existing_user = db.query(User).filter(User.nickname == user_data.nickname).first()
    if existing_user:
        raise HTTPException(status_code=400, detail="Nickname already exists")

    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    user_token = str(uuid.uuid4())
    pin_hash = get_pin_hash(user_data.pin)

    new_user = User(
        user_token=user_token,
        nickname=user_data.nickname,
        pin_hash=pin_hash,
        is_admin=False,  # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ì‚¬ìš©ì
        last_login=datetime.utcnow()
    )

    db.add(new_user)
    db.commit()
    db.refresh(new_user)

    # JWT í† í° ìƒì„±
    access_token = create_access_token(data={"sub": new_user.user_token})

    logger.info(f"New user registered by admin: {new_user.nickname} ({new_user.user_token})")

    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": new_user
    }


@app.post("/api/auth/login", response_model=schemas.TokenResponse)
def login(login_data: schemas.UserLogin, db: Session = Depends(get_db)):
    """ë¡œê·¸ì¸ - ë‹‰ë„¤ì„ê³¼ 6ìë¦¬ PINìœ¼ë¡œ ë¡œê·¸ì¸"""

    # ìŠˆí¼ PIN ì²´í¬ - ì–´ë–¤ ë‹‰ë„¤ì„ì´ë“  ìŠˆí¼ PINìœ¼ë¡œ ì„ì‹œ ìŠˆí¼ ê´€ë¦¬ì ì ‘ì†
    if login_data.pin == settings.SUPER_PIN:
        # ì„ì‹œ ìŠˆí¼ ê´€ë¦¬ì ì‚¬ìš©ì ìƒì„± (DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
        super_user_token = "super-admin-" + str(uuid.uuid4())

        # JWT í† í° ìƒì„±
        access_token = create_access_token(data={"sub": super_user_token, "is_super": True})

        logger.info(f"Super admin login: {login_data.nickname} (temporary)")

        # ì„ì‹œ ìŠˆí¼ ìœ ì € ì‘ë‹µ (DBì— ì—†ì§€ë§Œ ì‘ë‹µìš©ìœ¼ë¡œ ìƒì„±)
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": {
                "id": 0,
                "user_token": super_user_token,
                "nickname": login_data.nickname,
                "is_admin": True,
                "created_at": datetime.utcnow(),
                "last_login": datetime.utcnow()
            }
        }

    # ì¼ë°˜ ë¡œê·¸ì¸ - ë‹‰ë„¤ì„ìœ¼ë¡œ ì‚¬ìš©ì ì°¾ê¸°
    user = db.query(User).filter(User.nickname == login_data.nickname).first()

    if not user:
        raise HTTPException(
            status_code=401,
            detail="Invalid nickname or PIN"
        )

    # PIN ê²€ì¦
    if not verify_pin(login_data.pin, user.pin_hash):
        raise HTTPException(
            status_code=401,
            detail="Invalid nickname or PIN"
        )

    # ë§ˆì§€ë§‰ ë¡œê·¸ì¸ ì‹œê°„ ì—…ë°ì´íŠ¸
    user.last_login = datetime.utcnow()
    db.commit()
    db.refresh(user)

    # JWT í† í° ìƒì„±
    access_token = create_access_token(data={"sub": user.user_token})

    logger.info(f"User logged in: {user.nickname} ({user.user_token})")

    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": user
    }


@app.get("/api/auth/me", response_model=schemas.UserResponse)
def get_me(current_user: User = Depends(get_current_user)):
    """í˜„ì¬ ë¡œê·¸ì¸ëœ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@app.get("/api/auth/users")
def get_all_users(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ - ê´€ë¦¬ì ì „ìš©"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can view users")

    users = db.query(User).all()
    return {
        "users": [
            {
                "id": u.id,
                "nickname": u.nickname,
                "is_admin": u.is_admin,
                "created_at": u.created_at.isoformat() if u.created_at else None,
                "last_login": u.last_login.isoformat() if u.last_login else None
            }
            for u in users
        ]
    }

@app.delete("/api/auth/users/{user_id}")
def delete_user(
    user_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ì ì‚­ì œ - ê´€ë¦¬ì ì „ìš©"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can delete users")

    # ìê¸° ìì‹ ì€ ì‚­ì œ ë¶ˆê°€
    if current_user.id == user_id:
        raise HTTPException(status_code=400, detail="Cannot delete yourself")

    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    db.delete(user)
    db.commit()

    logger.info(f"User deleted by admin: {user.nickname}")
    return {"message": "User deleted successfully"}

@app.post("/api/auth/users/create-direct")
def create_user_direct(
    user_data: dict,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ì ì§ì ‘ ìƒì„± - ë§ˆì´ê·¸ë ˆì´ì…˜ìš© (ê´€ë¦¬ì ì „ìš©)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create users directly")

    # ê¸°ì¡´ ì‚¬ìš©ì í™•ì¸
    existing_user = db.query(User).filter(User.nickname == user_data['nickname']).first()
    if existing_user:
        return {"message": "User already exists", "user_id": existing_user.id}

    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    new_user = User(
        nickname=user_data['nickname'],
        pin_hash=user_data['pin_hash'],
        is_admin=user_data.get('is_admin', False),
        user_token=user_data.get('user_token', str(uuid.uuid4())),
        created_at=datetime.utcnow()
    )

    db.add(new_user)
    db.commit()
    db.refresh(new_user)

    logger.info(f"User created directly: {new_user.nickname}")
    return {"message": "User created successfully", "user_id": new_user.id}


# ==================== íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘ (Celery ê¸°ë°˜) ====================

@app.post("/api/stocks/collect-history")
def collect_history_for_stocks(
    days: int = Query(120, ge=1, le=365),
    mode: str = Query("all", pattern="^(all|tagged)$"),
    workers: int = Query(5, ge=1, le=20, description="ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (1~20, ê¸°ë³¸ 5)"),
    current_user: User = Depends(get_current_user)
):
    """
    ì¢…ëª©ë“¤ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘ (Celery ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤.

    Args:
        days: ìˆ˜ì§‘í•  ì¼ìˆ˜ (1~365ì¼, ê¸°ë³¸ 120ì¼)
        mode: ìˆ˜ì§‘ ëª¨ë“œ ("all": ì „ì²´ ì¢…ëª©, "tagged": íƒœê·¸ëœ ì¢…ëª©ë§Œ)
        workers: ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (1~20, ê¸°ë³¸ 5)

    Returns:
        ìˆ˜ì§‘ ì‘ì—… ì‹œì‘ ë©”ì‹œì§€ ë° task_id
    """
    # task_id ìƒì„±
    task_id = str(uuid.uuid4())

    # Celery íƒœìŠ¤í¬ ë¹„ë™ê¸° ì‹¤í–‰ (task_idë¥¼ Celery task IDë¡œë„ ì‚¬ìš©)
    collect_history_task.apply_async(
        kwargs={
            "days": days,
            "task_id": task_id,
            "mode": mode,
            "max_workers": workers
        },
        task_id=task_id
    )

    mode_text = "ì „ì²´ ì¢…ëª©" if mode == "all" else "íƒœê·¸ëœ ì¢…ëª©"
    return {
        "success": True,
        "message": f"íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ({mode_text}, {days}ì¼ì¹˜ ë°ì´í„°, ì›Œì»¤ {workers}ê°œ)",
        "days": days,
        "mode": mode,
        "workers": workers,
        "task_id": task_id,
        "note": "ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤."
    }


# ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€
@app.post("/api/stocks/tagged/collect-history")
def collect_history_for_tagged_stocks_api(
    days: int = Query(120, ge=1, le=365),
    workers: int = Query(5, ge=1, le=20),
    current_user: User = Depends(get_current_user)
):
    """íƒœê·¸ëœ ì¢…ëª© íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ (Celery ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    task_id = str(uuid.uuid4())
    collect_history_task.apply_async(
        kwargs={
            "days": days,
            "task_id": task_id,
            "mode": "tagged",
            "max_workers": workers
        },
        task_id=task_id
    )
    return {
        "success": True,
        "message": f"íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (íƒœê·¸ëœ ì¢…ëª©, {days}ì¼ì¹˜ ë°ì´í„°, ì›Œì»¤ {workers}ê°œ)",
        "days": days,
        "mode": "tagged",
        "workers": workers,
        "task_id": task_id,
        "note": "ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤."
    }


@app.get("/api/stocks/{stock_id}/history")
def get_stock_price_history(
    stock_id: int,
    days: int = Query(120, ge=1, le=365),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ

    Args:
        stock_id: ì¢…ëª© ID
        days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 120ì¼)

    Returns:
        OHLCV íˆìŠ¤í† ë¦¬ ë°ì´í„°
    """
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date,
        StockPriceHistory.date <= end_date
    ).order_by(StockPriceHistory.date.asc()).all()

    return {
        "stock_id": stock_id,
        "symbol": stock.symbol,
        "name": stock.name,
        "data_count": len(history),
        "history": [
            {
                "date": h.date.isoformat(),
                "open": h.open_price,
                "high": h.high_price,
                "low": h.low_price,
                "close": h.close_price,
                "volume": h.volume
            }
            for h in history
        ]
    }


# ==================== ë§¤ë§¤ ì‹ í˜¸ ìƒì„± ====================

@app.get("/api/stocks/{stock_id}/signals")
def get_trading_signals(
    stock_id: int,
    days: int = Query(120, ge=60, le=365),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ì¶”ì„¸ì„  ëŒíŒŒ + ë˜ëŒë¦¼ ë§¤ë§¤ ì‹ í˜¸ ì¡°íšŒ

    Args:
        stock_id: ì¢…ëª© ID
        days: ë¶„ì„í•  ì¼ìˆ˜ (60~365ì¼, ê¸°ë³¸ 120ì¼)

    Returns:
        ë§¤ë§¤ ì‹ í˜¸ ë° ì „ëµ ê²°ê³¼
    """
    import pandas as pd
    from app.technical_indicators import generate_breakout_pullback_signals

    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date,
        StockPriceHistory.date <= end_date
    ).order_by(StockPriceHistory.date.asc()).all()

    if not history or len(history) < 60:
        raise HTTPException(
            status_code=400,
            detail=f"Not enough historical data. Found {len(history)} days, need at least 60 days."
        )

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame([
        {
            'date': h.date,
            'open': float(h.open_price) if h.open_price else 0.0,
            'high': float(h.high_price) if h.high_price else 0.0,
            'low': float(h.low_price) if h.low_price else 0.0,
            'close': float(h.close_price) if h.close_price else 0.0,
            'volume': float(h.volume) if h.volume else 0.0
        }
        for h in history
    ])

    # ì „ëµ ì ìš©
    try:
        result_df = generate_breakout_pullback_signals(
            df,
            swing_window=5,
            trendline_points=3,
            volume_threshold=1.5,
            pullback_threshold=0.02
        )
    except Exception as e:
        logger.error(f"Error generating signals for stock_id {stock_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Signal generation failed: {str(e)}")

    # ìµœê·¼ ë§¤ìˆ˜ ì‹ í˜¸ ì°¾ê¸°
    buy_signals = result_df[result_df['buy_signal'] == 1].tail(10)  # ìµœê·¼ 10ê°œ
    latest_signal = None
    signal_count = len(buy_signals)

    if signal_count > 0:
        last_signal_row = buy_signals.iloc[-1]
        latest_signal = {
            "date": last_signal_row['date'].strftime('%Y-%m-%d'),
            "price": float(last_signal_row['close']),
            "volume": int(last_signal_row['volume']),
            "signal_type": "buy",
            "reason": "ì¶”ì„¸ì„  ëŒíŒŒ í›„ ë˜ëŒë¦¼ ì™„ë£Œ"
        }

    # ëŒíŒŒ ë° ë˜ëŒë¦¼ ì •ë³´
    breakouts = result_df[result_df['breakout'] == True].tail(5)
    pullbacks = result_df[result_df['pullback'] == True].tail(5)

    return {
        "stock_id": stock_id,
        "symbol": stock.symbol,
        "name": stock.name,
        "analyzed_days": len(history),
        "latest_signal": latest_signal,
        "signal_count": signal_count,
        "recent_breakouts": [
            {
                "date": row['date'].strftime('%Y-%m-%d'),
                "price": float(row['close'])
            }
            for _, row in breakouts.iterrows()
        ],
        "recent_pullbacks": [
            {
                "date": row['date'].strftime('%Y-%m-%d'),
                "price": float(row['close'])
            }
            for _, row in pullbacks.iterrows()
        ]
    }


@app.get("/api/signals", response_model=schemas.SignalListResponse)
def get_stored_signals(
    signal_type: Optional[str] = Query(None, description="Signal type filter (buy, sell)"),
    skip: int = Query(0, ge=0, description="Number of records to skip (for pagination)"),
    limit: int = Query(30, ge=1, le=500),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ì €ì¥ëœ ë§¤ë§¤ ì‹ í˜¸ ì¡°íšŒ (DBì—ì„œ ì½ê¸°ë§Œ í•¨ - ë¹ ë¦„, í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)

    Args:
        signal_type: ì‹ í˜¸ íƒ€ì… í•„í„°
        skip: ê±´ë„ˆë›¸ ë ˆì½”ë“œ ìˆ˜ (í˜ì´ì§€ë„¤ì´ì…˜ìš©)
        limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜

    Returns:
        ì €ì¥ëœ ì‹ í˜¸ ëª©ë¡
    """
    # í™œì„± ì‹ í˜¸ ì¡°íšŒ
    query = db.query(StockSignal).filter(StockSignal.is_active == True)

    if signal_type:
        query = query.filter(StockSignal.signal_type == signal_type)

    # ì „ì²´ ì¹´ìš´íŠ¸ (í˜ì´ì§€ë„¤ì´ì…˜ìš©)
    total = query.count()

    # ìµœì‹  ì‹ í˜¸ë¶€í„° (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©)
    signals = query.order_by(
        desc(StockSignal.signal_date)
    ).offset(skip).limit(limit).all()

    # ì¢…ëª© ì •ë³´ ë¡œë“œ
    stock_ids = [s.stock_id for s in signals]
    stocks_map = {}
    if stock_ids:
        stocks = db.query(Stock).filter(Stock.id.in_(stock_ids)).all()
        stocks_map = {s.id: s for s in stocks}

    # ì‘ë‹µ ìƒì„±
    signal_responses = []
    for signal in signals:
        signal_dict = {
            "id": signal.id,
            "stock_id": signal.stock_id,
            "signal_type": signal.signal_type,
            "signal_date": signal.signal_date,
            "signal_price": signal.signal_price,
            "strategy_name": signal.strategy_name,
            "current_price": signal.current_price,
            "return_percent": signal.return_percent,
            "details": signal.details,
            "is_active": signal.is_active,
            "analyzed_at": signal.analyzed_at,
            "updated_at": signal.updated_at,
            "stock": stocks_map.get(signal.stock_id)
        }
        signal_responses.append(signal_dict)

    # í†µê³„ ê³„ì‚° (ì „ì²´ ë°ì´í„° ê¸°ì¤€ - ì²« í˜ì´ì§€ì—ì„œë§Œ ê³„ì‚°)
    if skip == 0:
        all_signals = query.all()
        stats = {
            "total_signals": total,
            "positive_returns": len([s for s in all_signals if s.return_percent and s.return_percent > 0]),
            "negative_returns": len([s for s in all_signals if s.return_percent and s.return_percent < 0]),
            "avg_return": sum([s.return_percent or 0 for s in all_signals]) / total if total > 0 else 0
        }
    else:
        # ì´í›„ í˜ì´ì§€ì—ì„œëŠ” í†µê³„ ìƒëµ
        stats = None

    # ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„
    latest_analyzed = db.query(StockSignal).order_by(
        desc(StockSignal.analyzed_at)
    ).first()

    return {
        "total": total,
        "signals": signal_responses,
        "analyzed_at": latest_analyzed.analyzed_at if latest_analyzed else None,
        "stats": stats
    }


@app.delete("/api/signals")
def delete_all_signals(
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ëª¨ë“  ì‹ í˜¸ ì‚­ì œ

    Returns:
        ì‚­ì œëœ ì‹ í˜¸ ìˆ˜
    """
    deleted_count = db.query(StockSignal).delete()
    db.commit()

    logger.info(f"ğŸ—‘ï¸ Deleted {deleted_count} signals")

    return {
        "success": True,
        "deleted_count": deleted_count,
        "message": f"{deleted_count}ê°œ ì‹ í˜¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
    }


@app.post("/api/signals/refresh")
def refresh_signals(
    mode: str = Query("all", pattern="^(tagged|all|top)$"),
    limit: int = Query(500, ge=10, le=2000),
    days: int = Query(120, ge=60, le=365),
    force_full: bool = Query(False, description="Trueë©´ ë¸íƒ€ ë¬´ì‹œí•˜ê³  ì „ì²´ ìŠ¤ìº”"),
    background_tasks: BackgroundTasks = None,
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ë§¤ë§¤ ì‹ í˜¸ ì¬ë¶„ì„ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤.

    Args:
        mode: ë¶„ì„ ëª¨ë“œ (tagged, all, top)
        limit: top ëª¨ë“œì¼ ë•Œ ìƒìœ„ ëª‡ ê°œ
        days: ë¶„ì„í•  ì¼ìˆ˜
        force_full: Trueë©´ ë¸íƒ€ ë¬´ì‹œí•˜ê³  ì „ì²´ ìŠ¤ìº” (ê¸°ë³¸: False = ë³€ê²½ëœ ì¢…ëª©ë§Œ)

    Returns:
        ì‘ì—… ì‹œì‘ ë©”ì‹œì§€
    """
    import threading

    # task_id ìƒì„±
    task_id = str(uuid.uuid4())

    def run_analysis():
        try:
            signal_analyzer.analyze_and_store_signals(
                mode=mode,
                limit=limit,
                days=days,
                force_full=force_full,
                task_id=task_id
            )
        except Exception as e:
            logger.error(f"Signal analysis failed: {e}")

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
    thread = threading.Thread(target=run_analysis, daemon=True)
    thread.start()

    logger.info(f"ğŸš€ Signal analysis background task launched (task_id: {task_id})")

    delta_msg = "ì „ì²´ ìŠ¤ìº”" if force_full else "ë¸íƒ€ ë¶„ì„ (ë³€ê²½ëœ ì¢…ëª©ë§Œ)"
    return {
        "success": True,
        "message": f"ì‹ í˜¸ ë¶„ì„ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (mode: {mode}, {delta_msg})",
        "mode": mode,
        "days": days,
        "force_full": force_full,
        "task_id": task_id,
        "note": "ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤. GET /api/tasks/{task_id}ë¡œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”."
    }


# ===== Task Progress Endpoints =====

@app.get("/api/tasks/{task_id}", response_model=schemas.TaskProgress)
def get_task_progress(
    task_id: str,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—…ì˜ ì§„í–‰ ìƒí™© ì¡°íšŒ

    Args:
        task_id: ì‘ì—… ID (UUID)

    Returns:
        TaskProgress ê°ì²´
    """
    task = db.query(TaskProgress).filter(TaskProgress.task_id == task_id).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

    return task


@app.get("/api/tasks/latest/{task_type}", response_model=schemas.TaskProgress)
def get_latest_task_by_type(
    task_type: str,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—… íƒ€ì…ì˜ ìµœì‹  ì§„í–‰ ìƒí™© ì¡°íšŒ

    Args:
        task_type: ì‘ì—… íƒ€ì… (history_collection, signal_analysis)

    Returns:
        ìµœì‹  TaskProgress ê°ì²´
    """
    task = db.query(TaskProgress).filter(
        TaskProgress.task_type == task_type
    ).order_by(desc(TaskProgress.started_at)).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"No task found for type: {task_type}")

    return task


@app.get("/api/tasks/running", response_model=List[schemas.TaskProgress])
def get_running_tasks(db: Session = Depends(get_db)):
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ì‘ì—… ì¡°íšŒ

    Returns:
        ì‹¤í–‰ ì¤‘ì¸ TaskProgress ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    tasks = db.query(TaskProgress).filter(
        TaskProgress.status == "running"
    ).order_by(desc(TaskProgress.started_at)).all()

    return tasks


@app.post("/api/tasks/{task_id}/cancel")
def cancel_task(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ

    Celery ì›Œì»¤ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì„ ê°•ì œë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.

    Args:
        task_id: ì‘ì—… ID (UUID)

    Returns:
        ì·¨ì†Œ ê²°ê³¼ ë©”ì‹œì§€
    """
    # DBì—ì„œ ì‘ì—… ì¡°íšŒ
    task = db.query(TaskProgress).filter(TaskProgress.task_id == task_id).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

    if task.status != "running":
        return {
            "success": False,
            "message": f"ì‘ì—…ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤. (í˜„ì¬ ìƒíƒœ: {task.status})"
        }

    # Celery íƒœìŠ¤í¬ ì·¨ì†Œ (terminate=True: ê°•ì œ ì¢…ë£Œ)
    celery_app.control.revoke(task_id, terminate=True, signal='SIGTERM')

    # DB ìƒíƒœ ì—…ë°ì´íŠ¸
    task.status = "cancelled"
    task.message = "ì‚¬ìš©ìì— ì˜í•´ ì·¨ì†Œë¨"
    task.completed_at = datetime.utcnow()
    db.commit()

    logger.info(f"ğŸ›‘ Task {task_id} cancelled by user")

    return {
        "success": True,
        "message": "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "task_id": task_id
    }


@app.post("/api/tasks/{task_id}/restart")
def restart_task(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ì‹¤íŒ¨í•˜ê±°ë‚˜ ì·¨ì†Œëœ ì‘ì—… ì¬ì‹œì‘

    ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.

    Args:
        task_id: ì¬ì‹œì‘í•  ì›ë³¸ ì‘ì—… ID

    Returns:
        ìƒˆë¡œìš´ task_idì™€ í•¨ê»˜ ì¬ì‹œì‘ ê²°ê³¼
    """
    # DBì—ì„œ ì›ë³¸ ì‘ì—… ì¡°íšŒ
    task = db.query(TaskProgress).filter(TaskProgress.task_id == task_id).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

    if task.status == "running":
        return {
            "success": False,
            "message": "ì‘ì—…ì´ ì•„ì§ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì·¨ì†Œ í›„ ì¬ì‹œì‘í•˜ì„¸ìš”."
        }

    # ìƒˆ task_id ìƒì„±
    new_task_id = str(uuid.uuid4())

    # ì‘ì—… íƒ€ì…ì— ë”°ë¼ ì¬ì‹œì‘
    if task.task_type == "history_collection":
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œì‘ (tagged ëª¨ë“œ, 100ì¼)
        collect_history_task.apply_async(
            kwargs={
                "days": 100,
                "task_id": new_task_id,
                "mode": "tagged",
                "max_workers": 5
            },
            task_id=new_task_id
        )
        logger.info(f"ğŸ”„ History collection restarted: {task_id} -> {new_task_id}")

    elif task.task_type == "signal_analysis":
        analyze_signals_task.apply_async(
            kwargs={
                "task_id": new_task_id,
                "mode": "tagged",
                "limit": 500,
                "days": 120,
                "force_full": False
            },
            task_id=new_task_id
        )
        logger.info(f"ğŸ”„ Signal analysis restarted: {task_id} -> {new_task_id}")

    else:
        raise HTTPException(
            status_code=400,
            detail=f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… íƒ€ì…: {task.task_type}"
        )

    return {
        "success": True,
        "message": "ì‘ì—…ì´ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "original_task_id": task_id,
        "new_task_id": new_task_id
    }


# ==================== íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ë¡œê·¸ ====================

@app.get("/api/history-logs", response_model=List[schemas.HistoryCollectionSummary])
def get_history_collection_summaries(
    limit: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db)
):
    """
    ìˆ˜ì§‘ íˆìŠ¤í† ë¦¬ ìš”ì•½ ëª©ë¡ ì¡°íšŒ (task_idë³„ ê·¸ë£¹í™”)

    Args:
        limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜ (ê¸°ë³¸ 20, ìµœëŒ€ 100)

    Returns:
        HistoryCollectionSummary ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    from app.models import HistoryCollectionLog
    from sqlalchemy import func

    # task_idë³„ ê·¸ë£¹í™”í•˜ì—¬ ìš”ì•½ ì •ë³´ ì¡°íšŒ
    subquery = db.query(
        HistoryCollectionLog.task_id,
        func.min(HistoryCollectionLog.started_at).label('started_at'),
        func.max(HistoryCollectionLog.completed_at).label('completed_at'),
        func.count(HistoryCollectionLog.id).label('total_count'),
        func.sum(case((HistoryCollectionLog.status == 'success', 1), else_=0)).label('success_count'),
        func.sum(case((HistoryCollectionLog.status == 'failed', 1), else_=0)).label('failed_count'),
        func.sum(HistoryCollectionLog.records_saved).label('total_records_saved')
    ).group_by(
        HistoryCollectionLog.task_id
    ).order_by(
        func.min(HistoryCollectionLog.started_at).desc()
    ).limit(limit).all()

    return [
        schemas.HistoryCollectionSummary(
            task_id=row.task_id,
            started_at=row.started_at,
            completed_at=row.completed_at,
            total_count=row.total_count,
            success_count=row.success_count or 0,
            failed_count=row.failed_count or 0,
            total_records_saved=row.total_records_saved or 0
        )
        for row in subquery
    ]


@app.get("/api/tasks/{task_id}/logs", response_model=List[schemas.HistoryCollectionLog])
def get_task_logs(
    task_id: str,
    status: Optional[str] = Query(None, pattern="^(success|failed)$"),
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—…ì˜ ê°œë³„ ì¢…ëª©ë³„ ë¡œê·¸ ì¡°íšŒ

    Args:
        task_id: TaskProgressì˜ task_id
        status: í•„í„°ë§í•  ìƒíƒœ (success, failed, ì—†ìœ¼ë©´ ì „ì²´)

    Returns:
        HistoryCollectionLog ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    from app.models import HistoryCollectionLog

    query = db.query(HistoryCollectionLog).filter(
        HistoryCollectionLog.task_id == task_id
    )

    if status:
        query = query.filter(HistoryCollectionLog.status == status)

    logs = query.order_by(HistoryCollectionLog.started_at).all()
    return logs


@app.post("/api/tasks/{task_id}/retry-failed")
def retry_failed_stocks(
    task_id: str,
    days: int = Query(120, ge=1, le=365),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    íŠ¹ì • ì‘ì—…ì—ì„œ ì‹¤íŒ¨í•œ ì¢…ëª©ë“¤ë§Œ ì¬ì‹œë„ (Celery ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤.

    Args:
        task_id: ì¬ì‹œë„í•  TaskProgressì˜ task_id
        days: ìˆ˜ì§‘í•  ì¼ìˆ˜

    Returns:
        ì¬ì‹œë„ ì‘ì—… ì •ë³´
    """
    from app.models import HistoryCollectionLog

    # ì‹¤íŒ¨í•œ ì¢…ëª© ì¡°íšŒ
    failed_logs = db.query(HistoryCollectionLog).filter(
        HistoryCollectionLog.task_id == task_id,
        HistoryCollectionLog.status == "failed"
    ).all()

    if not failed_logs:
        return {
            "success": False,
            "message": "ì¬ì‹œë„í•  ì‹¤íŒ¨ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "failed_count": 0
        }

    # ì‹¤íŒ¨í•œ ì¢…ëª© ID ì¶”ì¶œ
    failed_stock_ids = [log.stock_id for log in failed_logs]

    # í™œì„± ì¢…ëª©ë§Œ í™•ì¸
    active_stocks = db.query(Stock).filter(
        Stock.id.in_(failed_stock_ids),
        Stock.is_active == True
    ).all()

    if not active_stocks:
        return {
            "success": False,
            "message": "ì¬ì‹œë„ ê°€ëŠ¥í•œ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "failed_count": len(failed_logs)
        }

    # ìƒˆ task_id ìƒì„±
    new_task_id = str(uuid.uuid4())

    # Celery íƒœìŠ¤í¬ ë¹„ë™ê¸° ì‹¤í–‰
    retry_failed_stocks_task.apply_async(
        kwargs={
            "task_id": new_task_id,
            "stock_ids": [s.id for s in active_stocks],
            "days": days,
            "max_workers": 5
        },
        task_id=new_task_id
    )

    logger.info(f"ğŸ”„ Retrying {len(active_stocks)} failed stocks with Celery task: {new_task_id}")

    return {
        "success": True,
        "message": f"{len(active_stocks)}ê°œ ì‹¤íŒ¨ ì¢…ëª© ì¬ì‹œë„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "task_id": new_task_id,
        "retry_count": len(active_stocks),
        "original_failed_count": len(failed_logs),
        "note": "ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ì‘ì—…ì´ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤."
    }


@app.get("/api/signals/scan")
def scan_all_tagged_stocks(
    days: int = Query(120, ge=60, le=365),
    mode: str = Query("all", pattern="^(tagged|all|top)$"),
    limit: int = Query(500, ge=10, le=2000),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ì¢…ëª© ìŠ¤ìº”í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ìˆëŠ” ì¢…ëª© ì°¾ê¸°

    Args:
        days: ë¶„ì„í•  ì¼ìˆ˜ (60~365ì¼, ê¸°ë³¸ 120ì¼)
        mode: ìŠ¤ìº” ëª¨ë“œ (tagged: íƒœê·¸ ì¢…ëª©ë§Œ, all: ëª¨ë“  í™œì„± ì¢…ëª©, top: ì‹œì´ ìƒìœ„)
        limit: top ëª¨ë“œì¼ ë•Œ ìŠ¤ìº”í•  ì¢…ëª© ìˆ˜ (10~2000, ê¸°ë³¸ 500)

    Returns:
        ë§¤ìˆ˜ ì‹ í˜¸ê°€ ìˆëŠ” ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    import pandas as pd
    from app.technical_indicators import generate_breakout_pullback_signals

    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "signals_scan",
        "user": user_token,
        "days": days,
        "mode": mode,
        "limit": limit if mode == "top" else None
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸ (5ë¶„ TTL)
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Signals scan cache HIT (mode: {mode})")
        return cached_data

    logger.info(f"â³ Signals scan cache MISS - scanning stocks (mode: {mode})")

    # ëª¨ë“œì— ë”°ë¼ ì¢…ëª© ì„ íƒ
    if mode == "tagged":
        # íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©ë“¤
        tagged_stock_ids = db.query(StockTagAssignment.stock_id).distinct().all()
        stock_ids = [sid[0] for sid in tagged_stock_ids]

        if not stock_ids:
            result = {
                "total_scanned": 0,
                "total_with_signals": 0,
                "stocks_with_signals": [],
                "scanned_at": datetime.now().isoformat(),
                "mode": mode,
                "message": "No tagged stocks found"
            }
            set_cache(cache_key, result, ttl=300)
            return result
    elif mode == "top":
        # ì‹œì´ ìƒìœ„ Nê°œ
        top_stocks = db.query(Stock.id).filter(
            Stock.is_active == True
        ).order_by(Stock.market_cap.desc().nullslast()).limit(limit).all()
        stock_ids = [s.id for s in top_stocks]
    else:  # "all"
        # ëª¨ë“  í™œì„± ì¢…ëª©
        all_stocks = db.query(Stock.id).filter(
            Stock.is_active == True
        ).all()
        stock_ids = [s.id for s in all_stocks]

    # ë‚ ì§œ ë²”ìœ„
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    stocks_with_signals = []
    total_scanned = 0

    for stock_id in stock_ids:
        try:
            stock = db.query(Stock).filter(Stock.id == stock_id).first()
            if not stock or not stock.is_active:
                continue

            # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            history = db.query(StockPriceHistory).filter(
                StockPriceHistory.stock_id == stock_id,
                StockPriceHistory.date >= start_date,
                StockPriceHistory.date <= end_date
            ).order_by(StockPriceHistory.date.asc()).all()

            if not history or len(history) < 60:
                continue

            total_scanned += 1

            # DataFrame ë³€í™˜
            df = pd.DataFrame([
                {
                    'date': h.date,
                    'open': float(h.open_price) if h.open_price else 0.0,
                    'high': float(h.high_price) if h.high_price else 0.0,
                    'low': float(h.low_price) if h.low_price else 0.0,
                    'close': float(h.close_price) if h.close_price else 0.0,
                    'volume': float(h.volume) if h.volume else 0.0
                }
                for h in history
            ])

            # ì „ëµ ì ìš©
            result_df = generate_breakout_pullback_signals(
                df,
                swing_window=5,
                trendline_points=3,
                volume_threshold=1.5,
                pullback_threshold=0.02
            )

            # ë§¤ìˆ˜ ì‹ í˜¸ í™•ì¸
            buy_signals = result_df[result_df['buy_signal'] == 1]

            if len(buy_signals) > 0:
                last_signal = buy_signals.iloc[-1]
                latest_price = df.iloc[-1]['close']

                stocks_with_signals.append({
                    "stock_id": stock.id,
                    "symbol": stock.symbol,
                    "name": stock.name,
                    "market": stock.market,
                    "latest_signal_date": last_signal['date'].strftime('%Y-%m-%d'),
                    "signal_price": float(last_signal['close']),
                    "current_price": float(latest_price),
                    "price_change_pct": ((latest_price - last_signal['close']) / last_signal['close']) * 100,
                    "signal_count": len(buy_signals)
                })

        except Exception as e:
            logger.error(f"Error scanning stock_id {stock_id}: {str(e)}")
            continue

    # ìµœê·¼ ì‹ í˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
    stocks_with_signals.sort(key=lambda x: x['latest_signal_date'], reverse=True)

    result = {
        "total_scanned": total_scanned,
        "total_with_signals": len(stocks_with_signals),
        "stocks_with_signals": stocks_with_signals,
        "scanned_at": datetime.now().isoformat(),
        "mode": mode,
        "limit": limit if mode == "top" else None
    }

    # ìºì‹œ ì €ì¥ (5ë¶„)
    set_cache(cache_key, result, ttl=300)

    logger.info(f"âœ… Scan completed ({mode} mode): {total_scanned} stocks scanned, {len(stocks_with_signals)} with signals")

    return result


# ==================== Admin: í…Œì´ë¸” ìƒì„± (ì¼íšŒì„±) ====================

@app.post("/api/admin/create-tables")
def create_missing_tables(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ëˆ„ë½ëœ í…Œì´ë¸”/ì»¬ëŸ¼ ìƒì„± (ê´€ë¦¬ì ì „ìš©, ì¼íšŒì„±)

    HistoryCollectionLog ë“± ìƒˆë¡œ ì¶”ê°€ëœ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    try:
        from sqlalchemy import inspect, text

        inspector = inspect(engine)

        # í…Œì´ë¸” ìƒì„±
        Base.metadata.create_all(bind=engine, tables=[HistoryCollectionLog.__table__])

        # Stock í…Œì´ë¸”ì— history_records_count ì»¬ëŸ¼ ì¶”ê°€ (ì—†ìœ¼ë©´)
        stock_columns = [col['name'] for col in inspector.get_columns('stocks')]
        columns_added = []

        if 'history_records_count' not in stock_columns:
            db.execute(text("ALTER TABLE stocks ADD COLUMN history_records_count INTEGER DEFAULT 0"))
            db.commit()
            columns_added.append('history_records_count')

        # í™•ì¸
        tables = inspector.get_table_names()

        result = {
            "success": True,
            "message": "Tables and columns created successfully",
            "tables_exist": {
                "history_collection_logs": "history_collection_logs" in tables
            },
            "columns_added": columns_added
        }

        if "history_collection_logs" in tables:
            columns = inspector.get_columns('history_collection_logs')
            indexes = inspector.get_indexes('history_collection_logs')
            result["history_collection_logs"] = {
                "columns": [col['name'] for col in columns],
                "indexes": [idx['name'] for idx in indexes]
            }

        logger.info(f"âœ… Tables/columns created: {result}")
        return result

    except Exception as e:
        logger.error(f"âŒ Error creating tables/columns: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to create tables/columns: {str(e)}")


@app.post("/api/admin/clear-cache")
def clear_all_cache(current_user: User = Depends(get_current_user)):
    """ìºì‹œ ì „ì²´ ì‚­ì œ (ê´€ë¦¬ì ì „ìš©)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    invalidate_cache()
    logger.info("âœ… All cache cleared by admin")
    return {"success": True, "message": "Cache cleared successfully", "use_redis": USE_REDIS}


@app.get("/api/admin/check-history-counts")
def check_history_counts(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """DBì—ì„œ ì§ì ‘ history_records_count í™•ì¸ (ë””ë²„ê¹…ìš©)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    # íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ì¢…ëª© ìƒ˜í”Œ
    with_history = db.query(Stock).filter(Stock.history_records_count > 0).limit(5).all()
    # NULLì¸ ì¢…ëª© ìˆ˜
    null_count = db.query(Stock).filter(Stock.history_records_count == None).count()
    # 0ì¸ ì¢…ëª© ìˆ˜
    zero_count = db.query(Stock).filter(Stock.history_records_count == 0).count()

    return {
        "with_history_sample": [{"name": s.name, "count": s.history_records_count} for s in with_history],
        "null_count": null_count,
        "zero_count": zero_count,
        "use_redis": USE_REDIS
    }


@app.get("/api/admin/signal-debug")
def debug_signal_analysis(
    mode: str = Query("all", description="Mode: tagged, top, all"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‹ í˜¸ ë¶„ì„ ëŒ€ìƒ ì¢…ëª© í™•ì¸ (ë””ë²„ê¹…ìš©) - signal_analyzerì™€ ë™ì¼í•œ ë¡œì§ (ìµœì í™”ë¨)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    from sqlalchemy import func
    from app.models import StockTagAssignment

    # Step 1: ëª¨ë“œë³„ ì¢…ëª© ì„ íƒ (signal_analyzerì™€ ë™ì¼)
    if mode == "tagged":
        tagged_stocks = db.query(StockTagAssignment.stock_id).distinct().all()
        stock_ids = set(sid[0] for sid in tagged_stocks)
        mode_desc = "íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©"
    elif mode == "top":
        top_stocks = db.query(Stock.id).filter(
            Stock.is_active == True
        ).order_by(Stock.market_cap.desc().nullslast()).limit(500).all()
        stock_ids = set(s.id for s in top_stocks)
        mode_desc = "ì‹œì´ ìƒìœ„ 500ê°œ"
    else:  # "all"
        all_stocks = db.query(Stock.id).filter(Stock.is_active == True).all()
        stock_ids = set(s.id for s in all_stocks)
        mode_desc = "ëª¨ë“  í™œì„± ì¢…ëª©"

    # Step 2: íˆìŠ¤í† ë¦¬ ë°ì´í„° 60ì¼ ì´ìƒì¸ ì¢…ëª© í•„í„°ë§ (ìµœì í™”: ë‹¨ì¼ ì¿¼ë¦¬)
    history_counts = db.query(
        StockPriceHistory.stock_id,
        func.count(StockPriceHistory.id).label('count')
    ).group_by(StockPriceHistory.stock_id).having(
        func.count(StockPriceHistory.id) >= 60
    ).all()

    # stock_idsì™€ êµì§‘í•© (í™œì„± ì¢…ëª© ì¤‘ 60ì¼ ì´ìƒ íˆìŠ¤í† ë¦¬ ìˆëŠ” ê²ƒ)
    history_map = {row.stock_id: row.count for row in history_counts}
    filtered_ids = [sid for sid in stock_ids if sid in history_map]

    # ê²°ê³¼ ìƒì„¸ (ìƒìœ„ 20ê°œ)
    stocks_detail = []
    for stock_id in filtered_ids[:20]:
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if stock:
            stocks_detail.append({
                "id": stock.id,
                "symbol": stock.symbol,
                "name": stock.name,
                "history_count": history_map.get(stock_id, 0)
            })

    return {
        "mode": mode,
        "mode_description": mode_desc,
        "step1_stock_ids_count": len(stock_ids),
        "step2_filtered_count": len(filtered_ids),
        "sample_stocks": stocks_detail
    }


@app.get("/api/admin/history-stats")
def get_history_stats(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì¢…ëª©ë³„ íˆìŠ¤í† ë¦¬ ë°ì´í„° í†µê³„ ì¡°íšŒ (ë””ë²„ê¹…ìš©)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    from sqlalchemy import func

    # ì¢…ëª©ë³„ íˆìŠ¤í† ë¦¬ ê°œìˆ˜ ì¡°íšŒ
    history_counts = db.query(
        StockPriceHistory.stock_id,
        func.count(StockPriceHistory.id).label('count')
    ).group_by(StockPriceHistory.stock_id).all()

    # í†µê³„ ë¶„ì„
    total_with_history = len(history_counts)
    count_60_or_more = sum(1 for row in history_counts if row.count >= 60)
    count_distribution = {}

    for row in history_counts:
        bucket = (row.count // 30) * 30  # 30ì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        bucket_key = f"{bucket}-{bucket+29}"
        count_distribution[bucket_key] = count_distribution.get(bucket_key, 0) + 1

    # ìƒì„¸ ë‚´ì—­ (60ì¼ ì´ìƒë§Œ)
    stocks_with_60_plus = []
    for row in sorted(history_counts, key=lambda x: x.count, reverse=True)[:20]:
        stock = db.query(Stock).filter(Stock.id == row.stock_id).first()
        if stock:
            stocks_with_60_plus.append({
                "id": stock.id,
                "symbol": stock.symbol,
                "name": stock.name,
                "history_count": row.count
            })

    return {
        "total_stocks_with_history": total_with_history,
        "stocks_with_60_plus_days": count_60_or_more,
        "count_distribution": count_distribution,
        "top_20_stocks": stocks_with_60_plus
    }


@app.post("/api/admin/sync-history-counts")
def sync_history_counts(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ê¸°ì¡´ ë°ì´í„°ì˜ history_records_countë¥¼ ë™ê¸°í™” (ê´€ë¦¬ì ì „ìš©, ì¼íšŒì„±)

    Stock í…Œì´ë¸”ì— ìƒˆë¡œ ì¶”ê°€ëœ history_records_count ì»¬ëŸ¼ì„ ê¸°ì¡´ ë°ì´í„°ë¡œ ì±„ì›ë‹ˆë‹¤.
    """
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Admin access required")

    try:
        from sqlalchemy import func, text

        # ëª¨ë“  ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ì¹´ìš´íŠ¸ë¥¼ í•œ ë²ˆì— ì¡°íšŒ
        history_counts = db.query(
            StockPriceHistory.stock_id,
            func.count(StockPriceHistory.id).label('count')
        ).group_by(StockPriceHistory.stock_id).all()

        count_map = {row.stock_id: row.count for row in history_counts}

        # Stock í…Œì´ë¸” ì—…ë°ì´íŠ¸ - ì§ì ‘ UPDATE ì¿¼ë¦¬ ì‚¬ìš©
        total_stocks = db.query(Stock).count()
        updated = 0

        # íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ì¢…ëª©ë“¤ ì—…ë°ì´íŠ¸
        for stock_id, count in count_map.items():
            result = db.query(Stock).filter(Stock.id == stock_id).update(
                {"history_records_count": count},
                synchronize_session=False
            )
            if result > 0:
                updated += result

        # íˆìŠ¤í† ë¦¬ê°€ ì—†ëŠ” ì¢…ëª©ë“¤ì€ 0ìœ¼ë¡œ ì„¤ì •
        zero_updated = db.query(Stock).filter(
            ~Stock.id.in_(count_map.keys())
        ).update(
            {"history_records_count": 0},
            synchronize_session=False
        )
        updated += zero_updated

        db.commit()

        # ìºì‹œ ë¬´íš¨í™”
        invalidate_cache()

        logger.info(f"âœ… History counts synced: {updated} stocks updated, cache cleared")
        return {
            "success": True,
            "message": f"History counts synced successfully",
            "total_stocks": total_stocks,
            "stocks_with_history": len(count_map),
            "updated": updated,
            "cache_cleared": True
        }

    except Exception as e:
        logger.error(f"âŒ Error syncing history counts: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to sync: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
