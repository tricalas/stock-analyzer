from fastapi import FastAPI, Depends, HTTPException, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import ORJSONResponse
from starlette.middleware.gzip import GZipMiddleware
from sqlalchemy.orm import Session
from sqlalchemy import desc
from typing import List, Optional
from datetime import datetime, date, timedelta
import logging
import hashlib
import json
from cachetools import TTLCache
import redis
import orjson

from app.config import settings
from app.database import engine, Base, get_db
from app.models import Stock, StockPrice, StockDailyData, StockPriceHistory, StockTag, StockTagAssignment, User, StockSignal, TaskProgress
from app import schemas
from app.crawlers.crawler_manager import CrawlerManager
from app.crawlers.price_history_crawler import price_history_crawler
from app.scheduler import stock_scheduler
from app.constants import ETF_KEYWORDS
from app.auth import get_pin_hash, verify_pin, create_access_token, get_current_user, get_optional_current_user
from app.signal_analyzer import signal_analyzer
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis ìºì‹œ ì„¤ì •
try:
    redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)
    redis_client.ping()
    logger.info("âœ… Redis connected successfully")
    USE_REDIS = True
except Exception as e:
    logger.warning(f"âš ï¸ Redis connection failed: {e}. Falling back to memory cache.")
    USE_REDIS = False
    # ë©”ëª¨ë¦¬ ìºì‹œ í´ë°± (TTL 300ì´ˆë¡œ ì¦ê°€)
    stocks_cache = TTLCache(maxsize=1000, ttl=300)

def get_cache(key: str):
    """ìºì‹œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if USE_REDIS:
        try:
            data = redis_client.get(f"stocks:{key}")
            if data:
                return orjson.loads(data)
            return None
        except Exception as e:
            logger.error(f"âŒ Redis get failed: {e}")
            return None
    else:
        return stocks_cache.get(key)

def set_cache(key: str, value: dict, ttl: int = 300):
    """ìºì‹œì— ë°ì´í„° ì €ì¥ (TTL: ê¸°ë³¸ 300ì´ˆ)"""
    if USE_REDIS:
        try:
            # orjson.dumps returns bytes, perfect for Redis
            redis_client.setex(f"stocks:{key}", ttl, orjson.dumps(value))
        except Exception as e:
            logger.error(f"âŒ Redis set failed: {e}")
    else:
        stocks_cache[key] = value

def invalidate_cache():
    """ëª¨ë“  ìºì‹œë¥¼ ë¬´íš¨í™” (íƒœê·¸ ë³€ê²½ ì‹œ í˜¸ì¶œ)"""
    if USE_REDIS:
        try:
            # Redisì˜ ëª¨ë“  stocks ê´€ë ¨ ìºì‹œ í‚¤ ì‚­ì œ
            for key in redis_client.scan_iter("stocks:*"):
                redis_client.delete(key)
            logger.info("âœ… Redis cache cleared")
        except Exception as e:
            logger.error(f"âŒ Redis cache clear failed: {e}")
    else:
        stocks_cache.clear()
        logger.info("âœ… Memory cache cleared")

Base.metadata.create_all(bind=engine)

# orjsonì„ ê¸°ë³¸ JSON serializerë¡œ ì‚¬ìš© (2-3ë°° ë¹ ë¦„)
app = FastAPI(
    title="Stock Analyzer API",
    version="1.0.0",
    default_response_class=ORJSONResponse
)

# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´ (ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ì†ë„ 2-3ë°° í–¥ìƒ)
app.add_middleware(GZipMiddleware, minimum_size=1000)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

crawler_manager = CrawlerManager()

# í¬ë¡¤ë§ ì¿¨íƒ€ì„ ê´€ë¦¬ (10ë¶„)
last_crawl_time = None
CRAWL_COOLDOWN_MINUTES = 10

# ê¸°ë³¸ íƒœê·¸ ì‹œë”©
def seed_default_tags(db: Session):
    """ê¸°ë³¸ íƒœê·¸ ë°ì´í„° ìƒì„± (ì‹œìŠ¤í…œ íƒœê·¸ëŠ” user_token=None)"""
    default_tags = [
        {
            "name": "favorite",
            "display_name": "ê´€ì‹¬",
            "color": "primary",
            "icon": "Star",
            "order": 0,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "dislike",
            "display_name": "ì œì™¸",
            "color": "loss",
            "icon": "ThumbsDown",
            "order": 99,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "owned",
            "display_name": "ë³´ìœ ",
            "color": "gain",
            "icon": "ShoppingCart",
            "order": 1,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "recommended",
            "display_name": "ì¶”ì²œ",
            "color": "primary",
            "icon": "ThumbsUp",
            "order": 2,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "watching",
            "display_name": "ê´€ì°°",
            "color": "muted",
            "icon": "Eye",
            "order": 3,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        },
        {
            "name": "error",
            "display_name": "ì—ëŸ¬",
            "color": "loss",
            "icon": "AlertCircle",
            "order": 98,
            "user_token": None  # ì‹œìŠ¤í…œ íƒœê·¸
        }
    ]

    # íƒœê·¸ê°€ í•˜ë‚˜ë„ ì—†ì„ ë•Œë§Œ ê¸°ë³¸ íƒœê·¸ ìƒì„± (ìµœì´ˆ 1íšŒ)
    existing_tags_count = db.query(StockTag).count()
    if existing_tags_count > 0:
        logger.info(f"Tags already exist ({existing_tags_count}), skipping seed")
        return

    for tag_data in default_tags:
        tag = StockTag(**tag_data)
        db.add(tag)
        logger.info(f"Created default tag: {tag_data['display_name']}")

    db.commit()
    logger.info("Default tags seeded successfully")


# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
@app.on_event("startup")
async def startup_event():
    stock_scheduler.start()
    logger.info("Stock scheduler started on application startup")

    # ê¸°ë³¸ íƒœê·¸ ìƒì„±
    db = next(get_db())
    try:
        seed_default_tags(db)
    finally:
        db.close()

@app.on_event("shutdown")
async def shutdown_event():
    stock_scheduler.stop()
    logger.info("Stock scheduler stopped on application shutdown")

@app.get("/")
def read_root():
    return {"message": "Stock Analyzer API", "version": "1.0.0"}

@app.get("/api/stocks", response_model=schemas.StockListResponse)
def get_stocks(
    market: Optional[str] = Query(None, description="Filter by market (KR, US)"),
    exchange: Optional[str] = Query(None, description="Filter by exchange"),
    sector: Optional[str] = Query(None, description="Filter by sector"),
    exclude_etf: bool = Query(False, description="Exclude ETF and index funds"),
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    order_by: Optional[str] = Query("market_cap", description="Sort field (market_cap, change_percent)"),
    order_dir: Optional[str] = Query("desc", description="Sort direction (asc, desc)"),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    # ìºì‹œ í‚¤ ìƒì„± (ìœ ì €ë³„, ì¡°ê±´ë³„ë¡œ êµ¬ë¶„)
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "user": user_token,
        "market": market,
        "exchange": exchange,
        "sector": sector,
        "exclude_etf": exclude_etf,
        "skip": skip,
        "limit": limit,
        "order_by": order_by,
        "order_dir": order_dir
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Cache HIT for {user_token[:8]}... {market=} {skip=}")
        return cached_data

    logger.info(f"â³ Cache MISS for {user_token[:8]}... {market=} {skip=}")

    query = db.query(Stock).filter(Stock.is_active == True)

    # 'ì œì™¸', 'ì—ëŸ¬', 'ì‚­ì œ' íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª© ì œì™¸ (ì‚¬ìš©ìë³„)
    if current_user:
        exclude_tags = db.query(StockTag).filter(
            StockTag.name.in_(["dislike", "error", "delete"])
        ).all()

        if exclude_tags:
            exclude_tag_ids = [tag.id for tag in exclude_tags]
            exclude_stock_ids = db.query(StockTagAssignment.stock_id).filter(
                StockTagAssignment.tag_id.in_(exclude_tag_ids),
                StockTagAssignment.user_token == current_user.user_token
            ).all()
            exclude_stock_ids = [sid[0] for sid in exclude_stock_ids]
            if exclude_stock_ids:
                query = query.filter(~Stock.id.in_(exclude_stock_ids))

    # ETF ë° ì§€ìˆ˜ ì¢…ëª© ì œì™¸
    if exclude_etf:
        for keyword in ETF_KEYWORDS:
            query = query.filter(~Stock.name.ilike(f'%{keyword}%'))

    if market:
        query = query.filter(Stock.market == market)
    if exchange:
        query = query.filter(Stock.exchange == exchange)
    if sector:
        query = query.filter(Stock.sector == sector)

    # ë™ì  ì •ë ¬: order_by, order_dir íŒŒë¼ë¯¸í„° ê¸°ë°˜
    # ë™ì¼ ê°’ì¼ ë•Œ ì¼ê´€ëœ ì •ë ¬ì„ ìœ„í•´ ë³´ì¡° í‚¤ ì¶”ê°€
    if order_by == "change_percent":
        if order_dir == "asc":
            query = query.order_by(
                Stock.change_percent.asc().nullslast(),
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )
        else:
            query = query.order_by(
                Stock.change_percent.desc().nullslast(),
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )
    else:
        # ê¸°ë³¸: ì‹œê°€ì´ì•¡ìˆœ
        if order_dir == "asc":
            query = query.order_by(
                Stock.market_cap.asc().nullslast(),
                Stock.id.asc()
            )
        else:
            query = query.order_by(
                Stock.market_cap.desc().nullslast(),
                Stock.id.asc()
            )

    # COUNT ìµœì í™”: ì²« í˜ì´ì§€(skip==0)ì—ì„œë§Œ ì •í™•í•œ count ê³„ì‚°
    # ì´í›„ í˜ì´ì§€ì—ì„œëŠ” ìºì‹œëœ ê°’ ì‚¬ìš© (3-5ë°° ì†ë„ í–¥ìƒ)
    if skip == 0:
        total = query.count()
    else:
        # ì´ì „ í˜ì´ì§€ì—ì„œ ìºì‹œëœ total ì‚¬ìš© (ì¶”ì •ì¹˜)
        # ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ count ê³„ì‚°
        count_cache_key = f"count:{hashlib.md5(orjson.dumps({**cache_key_data, 'skip': 0}, option=orjson.OPT_SORT_KEYS)).hexdigest()}"
        cached_first_page = get_cache(count_cache_key)
        if cached_first_page and 'total' in cached_first_page:
            total = cached_first_page['total']
        else:
            total = query.count()

    stocks = query.offset(skip).limit(limit).all()

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„) - íƒœê·¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ
    tags_map = {}
    latest_tag_dates = {}
    if current_user:
        from sqlalchemy import and_

        # ëª¨ë“  ì£¼ì‹ì˜ íƒœê·¸ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            and_(
                StockTagAssignment.stock_id.in_(stock_ids),
                StockTagAssignment.user_token == current_user.user_token
            )
        ).order_by(StockTagAssignment.created_at.desc()).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
                latest_tag_dates[ta.stock_id] = ta.created_at
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ ë°˜í™˜
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨ (ê°„ë‹¨ ê³„ì‚°)
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡ (ì´ë¯¸ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©)
        tags = []
        latest_tag_date = None
        if current_user and stock.id in tags_map:
            latest_tag_date = latest_tag_dates.get(stock.id)
            tags = [tags_by_id.get(ta.tag_id) for ta in tags_map[stock.id]]
            tags = [t for t in tags if t is not None]

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": latest_tag_date,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ í•„ë“œë§Œ ìœ ì§€
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥
    result = {
        "total": total,
        "stocks": stock_list,
        "page": skip // limit + 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=300)  # 5ë¶„ ìºì‹œ
    return result

@app.get("/api/stocks/search", response_model=schemas.StockListResponse)
def search_stocks(
    q: str = Query(..., min_length=1, description="Search query (name or symbol)"),
    market: Optional[str] = Query(None, description="Filter by market (KR, US)"),
    limit: int = Query(10, ge=1, le=50),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """ì¢…ëª© ê²€ìƒ‰ API - ì¢…ëª©ëª… ë˜ëŠ” ì‹¬ë³¼ë¡œ ê²€ìƒ‰ (ìë™ì™„ì„±ìš©)"""
    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "search",
        "user": user_token,
        "q": q.lower(),
        "market": market,
        "limit": limit
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Search cache HIT for '{q}'")
        return cached_data

    logger.info(f"â³ Search cache MISS for '{q}'")

    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
    query = db.query(Stock).filter(Stock.is_active == True)

    # 'ì œì™¸', 'ì—ëŸ¬', 'ì‚­ì œ' íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª© ì œì™¸ (ì‚¬ìš©ìë³„)
    if current_user:
        exclude_tags = db.query(StockTag).filter(
            StockTag.name.in_(["dislike", "error", "delete"])
        ).all()

        if exclude_tags:
            exclude_tag_ids = [tag.id for tag in exclude_tags]
            exclude_stock_ids = db.query(StockTagAssignment.stock_id).filter(
                StockTagAssignment.tag_id.in_(exclude_tag_ids),
                StockTagAssignment.user_token == current_user.user_token
            ).all()
            exclude_stock_ids = [sid[0] for sid in exclude_stock_ids]
            if exclude_stock_ids:
                query = query.filter(~Stock.id.in_(exclude_stock_ids))

    # ì¢…ëª©ëª… ë˜ëŠ” ì‹¬ë³¼ë¡œ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    search_filter = (
        Stock.name.ilike(f'%{q}%') |
        Stock.symbol.ilike(f'%{q}%')
    )
    query = query.filter(search_filter)

    # ë§ˆì¼“ í•„í„°
    if market:
        query = query.filter(Stock.market == market)

    # ì‹œê°€ì´ì•¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê²€ìƒ‰ ê²°ê³¼ì—ì„œë„ í° ê¸°ì—…ì´ ë¨¼ì €)
    query = query.order_by(
        Stock.market_cap.desc().nullslast(),
        Stock.id.asc()
    )

    # ì œí•œëœ ìˆ˜ë§Œ ê°€ì ¸ì˜¤ê¸° (ìë™ì™„ì„±ìš©)
    stocks = query.limit(limit).all()
    total = len(stocks)

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„)
    tags_map = {}
    latest_tag_dates = {}
    if current_user and stocks:
        from sqlalchemy import and_

        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            and_(
                StockTagAssignment.stock_id.in_(stock_ids),
                StockTagAssignment.user_token == current_user.user_token
            )
        ).order_by(StockTagAssignment.created_at.desc()).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
                latest_tag_dates[ta.stock_id] = ta.created_at
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡
        tags = []
        latest_tag_date = None
        if current_user and stock.id in tags_map:
            latest_tag_date = latest_tag_dates.get(stock.id)
            tags = [tags_by_id.get(ta.tag_id) for ta in tags_map[stock.id]]
            tags = [t for t in tags if t is not None]

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": latest_tag_date,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ ìµœì†Œ í•„ë“œ
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥ (ê²€ìƒ‰ì€ 1ë¶„ ìºì‹œ)
    result = {
        "total": total,
        "stocks": stock_list,
        "page": 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=60)  # 1ë¶„ ìºì‹œ
    return result


@app.get("/api/stocks/{stock_id}", response_model=schemas.Stock)
def get_stock(stock_id: int, db: Session = Depends(get_db)):
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")
    return stock

@app.get("/api/stocks/{stock_id}/prices", response_model=List[schemas.StockPrice])
def get_stock_prices(
    stock_id: int,
    start_date: Optional[date] = Query(None),
    end_date: Optional[date] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(StockPrice).filter(StockPrice.stock_id == stock_id)

    if start_date:
        query = query.filter(StockPrice.date >= start_date)
    if end_date:
        query = query.filter(StockPrice.date <= end_date)

    prices = query.order_by(StockPrice.date.desc()).limit(500).all()
    return prices

@app.get("/api/stocks/{stock_id}/daily-data", response_model=List[schemas.StockDailyData])
def get_stock_daily_data(
    stock_id: int,
    start_date: Optional[date] = Query(None),
    end_date: Optional[date] = Query(None),
    db: Session = Depends(get_db)
):
    query = db.query(StockDailyData).filter(StockDailyData.stock_id == stock_id)

    if start_date:
        query = query.filter(StockDailyData.date >= start_date)
    if end_date:
        query = query.filter(StockDailyData.date <= end_date)

    daily_data = query.order_by(StockDailyData.date.desc()).limit(500).all()
    return daily_data

def run_background_crawl(market: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  í¬ë¡¤ë§ ì‘ì—…"""
    try:
        logger.info(f"Starting background crawl for market: {market}")
        result = crawler_manager.update_stock_list(market)

        # ETF í•„í„°ë§ ì •ë³´ í¬í•¨í•œ ë©”ì‹œì§€ ìƒì„±
        etf_info = ""
        if result.get('skipped_etf', 0) > 0:
            etf_info = f" ({result['skipped_etf']} ETF/Index stocks filtered out)"

        logger.info(f"Background crawl completed: {result['success']} out of {result['total']} stocks{etf_info}")

        # ìºì‹œ ë¬´íš¨í™”
        if redis_client:
            try:
                # ëª¨ë“  stocks ê´€ë ¨ ìºì‹œ ì‚­ì œ
                for key in redis_client.scan_iter("cache:*stocks*"):
                    redis_client.delete(key)
                logger.info("Cache invalidated after crawling")
            except Exception as e:
                logger.error(f"Failed to invalidate cache: {e}")

    except Exception as e:
        logger.error(f"Error during background stock list crawling: {str(e)}")


@app.post("/api/crawl/stocks", response_model=schemas.CrawlingStatus)
def crawl_stock_list(
    background_tasks: BackgroundTasks,
    market: str = Query("ALL", pattern="^(ALL|KR|US)$"),
    current_user: User = Depends(get_current_user)
):
    """ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§ - 10ë¶„ ì¿¨íƒ€ì„ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)"""
    global last_crawl_time

    # ì¿¨íƒ€ì„ ì²´í¬
    if last_crawl_time:
        elapsed_time = datetime.utcnow() - last_crawl_time
        remaining_seconds = (CRAWL_COOLDOWN_MINUTES * 60) - elapsed_time.total_seconds()

        if remaining_seconds > 0:
            remaining_minutes = int(remaining_seconds // 60)
            remaining_secs = int(remaining_seconds % 60)
            raise HTTPException(
                status_code=429,
                detail=f"í¬ë¡¤ë§ ì¿¨íƒ€ì„ì…ë‹ˆë‹¤. {remaining_minutes}ë¶„ {remaining_secs}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )

    # ì¿¨íƒ€ì„ ì—…ë°ì´íŠ¸
    last_crawl_time = datetime.utcnow()

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€
    background_tasks.add_task(run_background_crawl, market)

    # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    return {
        "success": 0,
        "failed": 0,
        "total": 0,
        "skipped_etf": 0,
        "message": f"í¬ë¡¤ë§ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œê¹Œì§€ ì•½ 20ì´ˆ ì†Œìš”ë©ë‹ˆë‹¤."
    }


@app.post("/api/crawl/indicators/{stock_id}")
def calculate_indicators(stock_id: int, db: Session = Depends(get_db)):
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    try:
        crawler_manager.calculate_technical_indicators(stock_id)
        return {"message": f"Successfully calculated indicators for {stock.symbol}"}
    except Exception as e:
        logger.error(f"Error calculating indicators: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/scheduler/status")
def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë° ë“±ë¡ëœ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        jobs = stock_scheduler.get_jobs()
        return {
            "running": stock_scheduler.scheduler.running,
            "jobs": jobs,
            "message": "Scheduler is running" if stock_scheduler.scheduler.running else "Scheduler is stopped"
        }
    except Exception as e:
        logger.error(f"Error getting scheduler status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/scheduler/trigger")
def trigger_manual_crawl():
    """ìˆ˜ë™ìœ¼ë¡œ ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰"""
    try:
        result = stock_scheduler.trigger_manual_crawl()
        return {
            "message": "Manual crawling completed successfully",
            "result": result
        }
    except Exception as e:
        logger.error(f"Error in manual crawling: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/scheduler/trigger-history")
def trigger_manual_history_collection(
    background_tasks: BackgroundTasks,
    days: int = Query(100, ge=1, le=365, description="Number of days to collect")
):
    """ìˆ˜ë™ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    def run_collection():
        try:
            logger.info(f"ğŸš€ Background history collection started ({days} days)...")
            result = stock_scheduler.trigger_manual_history_collection(days=days)
            logger.info(f"âœ… Background history collection completed: {result}")
        except Exception as e:
            logger.error(f"âŒ Error in background history collection: {str(e)}")

    background_tasks.add_task(run_collection)

    return {
        "success": True,
        "message": f"íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ ({days}ì¼ì¹˜ ë°ì´í„°)",
        "days": days,
        "mode": settings.HISTORY_COLLECTION_MODE,
        "note": "Check Railway logs for progress"
    }

@app.get("/api/stocks/{stock_id}/price-history", response_model=List[schemas.StockPriceHistory])
def get_stock_price_history(
    stock_id: int,
    days: int = Query(30, ge=1, le=365, description="Number of days to retrieve"),
    db: Session = Depends(get_db)
):
    """íŠ¹ì • ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ìµœê·¼ Nì¼ ë°ì´í„° ì¡°íšŒ
    from datetime import date, timedelta
    start_date = date.today() - timedelta(days=days)

    price_history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date
    ).order_by(StockPriceHistory.date.desc()).all()

    return price_history

@app.post("/api/stocks/{stock_id}/crawl-history")
def crawl_stock_price_history(
    stock_id: int,
    days: int = Query(100, ge=1, le=365, description="Number of days to crawl"),
    db: Session = Depends(get_db)
):
    """ê°œë³„ ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ í¬ë¡¤ë§"""
    try:
        # ì¢…ëª© ì¡´ì¬ í™•ì¸
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        logger.info(f"Starting price history crawling for stock {stock.symbol}")

        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        latest_record = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).order_by(StockPriceHistory.date.desc()).first()

        # í¬ë¡¤ë§ ì‹¤í–‰
        price_data = price_history_crawler.fetch_price_history(stock.symbol, days)

        if not price_data:
            return {
                "success": 0,
                "failed": 1,
                "total": 1,
                "message": f"No price data found for {stock.symbol}"
            }

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        success_count = 0
        failed_count = 0

        for data in price_data:
            try:
                # ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                existing = db.query(StockPriceHistory).filter(
                    StockPriceHistory.stock_id == stock_id,
                    StockPriceHistory.date == data['date']
                ).first()

                if existing:
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    existing.open_price = data['open_price']
                    existing.high_price = data['high_price']
                    existing.low_price = data['low_price']
                    existing.close_price = data['close_price']
                    existing.volume = data['volume']
                    existing.updated_at = datetime.utcnow()
                else:
                    # ìƒˆ ë°ì´í„° ì¶”ê°€
                    new_record = StockPriceHistory(
                        stock_id=stock_id,
                        **data
                    )
                    db.add(new_record)

                success_count += 1

            except Exception as e:
                logger.error(f"Error saving price data for {data['date']}: {str(e)}")
                failed_count += 1
                continue

        # ì»¤ë°‹
        db.commit()

        logger.info(f"Price history crawling completed for {stock.symbol}: {success_count} success, {failed_count} failed")

        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(price_data),
            "message": f"Successfully crawled {success_count} price records for {stock.symbol}"
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error crawling price history for stock {stock_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stocks/{stock_id}/history-status")
def get_stock_history_status(stock_id: int, db: Session = Depends(get_db)):
    """ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ìƒíƒœ í™•ì¸"""
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # íˆìŠ¤í† ë¦¬ ë°ì´í„° í†µê³„
    total_records = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).count()

    latest_record = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).order_by(StockPriceHistory.date.desc()).first()

    oldest_record = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id
    ).order_by(StockPriceHistory.date.asc()).first()

    return {
        "stock_symbol": stock.symbol,
        "stock_name": stock.name,
        "total_records": total_records,
        "latest_date": latest_record.date if latest_record else None,
        "oldest_date": oldest_record.date if oldest_record else None,
        "has_data": total_records > 0
    }

@app.delete("/api/stocks/cleanup-etf")
def cleanup_etf_stocks(db: Session = Depends(get_db)):
    """ì§€ìˆ˜/ETF ì¢…ëª©ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì™„ì „íˆ ì‚­ì œ"""
    try:
        # ì‚­ì œí•  ì¢…ëª©ë“¤ ì°¾ê¸°
        etf_stocks = []
        for keyword in ETF_KEYWORDS:
            stocks = db.query(Stock).filter(Stock.name.ilike(f'%{keyword}%')).all()
            etf_stocks.extend(stocks)

        # ì¤‘ë³µ ì œê±°
        unique_etf_stocks = list({stock.id: stock for stock in etf_stocks}.values())

        logger.info(f"Found {len(unique_etf_stocks)} ETF/Index stocks to delete")

        deleted_count = 0
        deleted_stocks = []

        for stock in unique_etf_stocks:
            try:
                # ê´€ë ¨ ë°ì´í„°ë„ í•¨ê»˜ ì‚­ì œ (cascadeë¡œ ìë™ ì‚­ì œë¨)
                deleted_stocks.append({
                    "symbol": stock.symbol,
                    "name": stock.name
                })

                db.delete(stock)
                deleted_count += 1

            except Exception as e:
                logger.error(f"Error deleting stock {stock.symbol}: {str(e)}")
                continue

        # ì»¤ë°‹
        db.commit()

        logger.info(f"Successfully deleted {deleted_count} ETF/Index stocks")

        return {
            "deleted_count": deleted_count,
            "deleted_stocks": deleted_stocks,
            "message": f"Successfully deleted {deleted_count} ETF/Index stocks from database"
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error during ETF cleanup: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/stocks/{stock_id}")
def delete_stock(stock_id: int, db: Session = Depends(get_db)):
    """
    ì¢…ëª©ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ë¥¼ ì™„ì „ ì‚­ì œ
    """
    try:
        # ì¢…ëª© ì¡´ì¬ í™•ì¸
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        # í•´ë‹¹ ì¢…ëª©ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì‚­ì œ
        history_count = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).count()

        db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock_id
        ).delete()

        # ì¢…ëª© ë°ì´í„° ì‚­ì œ
        db.delete(stock)
        db.commit()

        logger.info(f"Deleted stock {stock.symbol} ({stock.name}) and {history_count} history records")

        return {
            "success": True,
            "message": f"ì¢…ëª© '{stock.name}({stock.symbol})'ê³¼ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_history_count": history_count
        }

    except Exception as e:
        db.rollback()
        logger.error(f"Error deleting stock {stock_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete stock: {str(e)}")

@app.post("/api/stocks/{stock_id}/analyze")
async def analyze_single_stock(
    stock_id: int,
    db: Session = Depends(get_db)
):
    """
    ë‹¨ì¼ ì¢…ëª© ë¶„ì„: ë„¤ì´ë²„ì—ì„œ ìƒì„¸ ì •ë³´ ë° ì¼ë³„ ê°€ê²© í¬ë¡¤ë§
    ì¤‘ë³µ ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
    """
    try:
        # ì¢…ëª© ì¡°íšŒ
        stock = db.query(Stock).filter(Stock.id == stock_id).first()
        if not stock:
            raise HTTPException(status_code=404, detail="Stock not found")

        logger.info(f"Analyzing stock: {stock.symbol} ({stock.name})")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        from app.crawlers.naver_us_crawler import NaverUSStockCrawler
        crawler = NaverUSStockCrawler()

        # ì¢…ëª© ë¶„ì„ ì‹¤í–‰
        # US ì£¼ì‹ì€ sector í•„ë“œì— reuters_code (ì˜ˆ: NVDA.O)ê°€ ì €ì¥ë˜ì–´ ìˆìŒ
        symbol_to_use = stock.sector if stock.market == "US" and stock.sector else stock.symbol
        result = crawler.analyze_single_stock(symbol_to_use)

        if not result['success']:
            raise HTTPException(status_code=500, detail=result['message'])

        stats = {
            'new_records': 0,
            'duplicate_records': 0,
            'updated_overview': False
        }

        # Overview ì •ë³´ ì—…ë°ì´íŠ¸
        if result['overview']:
            overview = result['overview']
            stock.current_price = overview.get('current_price', stock.current_price)
            stock.change_amount = overview.get('change_amount', stock.change_amount)
            stock.change_percent = overview.get('change_percent', stock.change_percent)
            stock.previous_close = overview.get('previous_close', stock.previous_close)
            stock.market_cap = overview.get('market_cap', stock.market_cap)
            stock.trading_volume = overview.get('volume', stock.trading_volume)
            stock.updated_at = datetime.utcnow()
            stats['updated_overview'] = True
            logger.info(f"Updated overview for {stock.symbol}")

        # ê°€ê²© íˆìŠ¤í† ë¦¬ ì €ì¥ (ì¤‘ë³µ ì²´í¬)
        if result['price_history']:
            for price_data in result['price_history']:
                try:
                    price_date = datetime.strptime(price_data['date'], '%Y%m%d').date()

                    # ì¤‘ë³µ ì²´í¬
                    existing_record = db.query(StockPriceHistory).filter(
                        StockPriceHistory.stock_id == stock.id,
                        StockPriceHistory.date == price_date
                    ).first()

                    if existing_record:
                        stats['duplicate_records'] += 1
                        continue

                    # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                    price_history = StockPriceHistory(
                        stock_id=stock.id,
                        date=price_date,
                        open_price=price_data['open_price'],
                        high_price=price_data['high_price'],
                        low_price=price_data['low_price'],
                        close_price=price_data['close_price'],
                        volume=price_data['volume']
                    )
                    db.add(price_history)
                    stats['new_records'] += 1

                except Exception as e:
                    logger.error(f"Error saving price record: {e}")
                    continue

        db.commit()

        # ìµœì‹  ê°±ì‹  ë‚ ì§œ ì¡°íšŒ
        latest_record = db.query(StockPriceHistory).filter(
            StockPriceHistory.stock_id == stock.id
        ).order_by(StockPriceHistory.date.desc()).first()

        return {
            "success": True,
            "stock_id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "stats": stats,
            "latest_update_date": latest_record.date.isoformat() if latest_record else None,
            "total_records": db.query(StockPriceHistory).filter(
                StockPriceHistory.stock_id == stock.id
            ).count(),
            "message": f"Successfully analyzed {stock.symbol}"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing stock {stock_id}: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to analyze stock: {str(e)}")

# ===== íƒœê·¸ ê´€ë¦¬ API =====

@app.get("/api/tags", response_model=schemas.TagListResponse)
def get_tags(db: Session = Depends(get_db)):
    """ëª¨ë“  íƒœê·¸ ëª©ë¡ ì¡°íšŒ"""
    tags = db.query(StockTag).filter(StockTag.is_active == True).order_by(StockTag.order).all()
    return {"tags": tags}

@app.post("/api/tags", response_model=schemas.StockTag)
def create_tag(
    tag: schemas.StockTagCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ìƒˆ íƒœê·¸ ìƒì„± - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create tags")

    # ì¤‘ë³µ ì²´í¬
    existing_tag = db.query(StockTag).filter(StockTag.name == tag.name).first()
    if existing_tag:
        raise HTTPException(status_code=400, detail="Tag with this name already exists")

    new_tag = StockTag(**tag.dict())
    db.add(new_tag)
    db.commit()
    db.refresh(new_tag)
    return new_tag

@app.put("/api/tags/{tag_id}", response_model=schemas.StockTag)
def update_tag(
    tag_id: int,
    tag: schemas.StockTagCreate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íƒœê·¸ ì—…ë°ì´íŠ¸ - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can update tags")

    existing_tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not existing_tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì´ë¦„ ë³€ê²½ ì‹œ ì¤‘ë³µ ì²´í¬
    if tag.name != existing_tag.name:
        duplicate = db.query(StockTag).filter(StockTag.name == tag.name).first()
        if duplicate:
            raise HTTPException(status_code=400, detail="Tag with this name already exists")

    # ì—…ë°ì´íŠ¸
    for key, value in tag.dict().items():
        setattr(existing_tag, key, value)

    db.commit()
    db.refresh(existing_tag)
    return existing_tag

@app.delete("/api/tags/{tag_id}")
def delete_tag(
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íƒœê·¸ ì‚­ì œ - ê´€ë¦¬ìë§Œ ê°€ëŠ¥"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can delete tags")

    tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # í• ë‹¹ëœ íƒœê·¸ë„ ëª¨ë‘ ì‚­ì œ (cascadeë¡œ ìë™ ì²˜ë¦¬ë¨)
    db.delete(tag)
    db.commit()

    return {"success": True, "message": f"Tag '{tag.display_name}' deleted successfully"}

@app.post("/api/stocks/{stock_id}/tags/{tag_id}", response_model=schemas.TagAssignmentResponse)
def add_tag_to_stock(
    stock_id: int,
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì¢…ëª©ì— íƒœê·¸ ì¶”ê°€ (ì‚¬ìš©ìë³„)"""
    # ì¢…ëª© ì¡´ì¬ í™•ì¸
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # íƒœê·¸ ì¡´ì¬ í™•ì¸
    tag = db.query(StockTag).filter(StockTag.id == tag_id).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì´ë¯¸ í• ë‹¹ëœ íƒœê·¸ì¸ì§€ í™•ì¸ (ì‚¬ìš©ìë³„)
    existing = db.query(StockTagAssignment).filter(
        StockTagAssignment.stock_id == stock_id,
        StockTagAssignment.tag_id == tag_id,
        StockTagAssignment.user_token == current_user.user_token
    ).first()

    if existing:
        return {"message": "Tag already assigned to this stock", "tag": tag}

    # ìƒˆ í• ë‹¹ ìƒì„± (ì‚¬ìš©ì í† í° í¬í•¨)
    assignment = StockTagAssignment(stock_id=stock_id, tag_id=tag_id, user_token=current_user.user_token)
    db.add(assignment)
    db.commit()

    # ìºì‹œ ë¬´íš¨í™”
    invalidate_cache()

    return {"message": f"Tag '{tag.display_name}' added to {stock.name}", "tag": tag}

@app.delete("/api/stocks/{stock_id}/tags/{tag_id}")
def remove_tag_from_stock(
    stock_id: int,
    tag_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì¢…ëª©ì—ì„œ íƒœê·¸ ì œê±° (ì‚¬ìš©ìë³„)"""
    assignment = db.query(StockTagAssignment).filter(
        StockTagAssignment.stock_id == stock_id,
        StockTagAssignment.tag_id == tag_id,
        StockTagAssignment.user_token == current_user.user_token
    ).first()

    if not assignment:
        raise HTTPException(status_code=404, detail="Tag assignment not found")

    db.delete(assignment)
    db.commit()

    # ìºì‹œ ë¬´íš¨í™”
    invalidate_cache()

    return {"message": "Tag removed from stock"}

@app.get("/api/stocks/by-tag/{tag_name}", response_model=schemas.StockListResponse)
def get_stocks_by_tag(
    tag_name: str,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """íŠ¹ì • íƒœê·¸ê°€ ë¶€ì—¬ëœ ì¢…ëª© ëª©ë¡ ì¡°íšŒ (ì‚¬ìš©ìë³„) - ìµœì í™”ë¨"""

    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "by-tag",
        "user": user_token,
        "tag_name": tag_name,
        "skip": skip,
        "limit": limit
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Cache HIT for tag {tag_name}, user {user_token[:8]}...")
        return cached_data

    logger.info(f"â³ Cache MISS for tag {tag_name}, user {user_token[:8]}...")

    # ì¸ì¦ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ê²°ê³¼
    if not current_user:
        result = {"total": 0, "stocks": [], "page": 1, "page_size": limit}
        set_cache(cache_key, result, ttl=300)
        return result

    # íƒœê·¸ ì°¾ê¸°
    tag = db.query(StockTag).filter(StockTag.name == tag_name).first()
    if not tag:
        raise HTTPException(status_code=404, detail="Tag not found")

    # ì¢…ëª© ì¡°íšŒ (JOINìœ¼ë¡œ í•œ ë²ˆì—)
    query = db.query(Stock).join(
        StockTagAssignment,
        (StockTagAssignment.stock_id == Stock.id) &
        (StockTagAssignment.tag_id == tag.id) &
        (StockTagAssignment.user_token == current_user.user_token)
    ).filter(Stock.is_active == True)

    # ì¼ê´€ëœ ì •ë ¬: ì‹œê°€ì´ì•¡ ë‚´ë¦¼ì°¨ìˆœ
    query = query.order_by(
        Stock.market_cap.desc().nullslast(),
        Stock.id.asc()
    )

    # COUNT ìµœì í™”: ì²« í˜ì´ì§€ì—ì„œë§Œ ì •í™•í•œ count ê³„ì‚°
    if skip == 0:
        total = query.count()
    else:
        # ì´ì „ í˜ì´ì§€ì—ì„œ ìºì‹œëœ total ì‚¬ìš©
        count_cache_key = f"count:{hashlib.md5(orjson.dumps({**cache_key_data, 'skip': 0}, option=orjson.OPT_SORT_KEYS)).hexdigest()}"
        cached_first_page = get_cache(count_cache_key)
        if cached_first_page and 'total' in cached_first_page:
            total = cached_first_page['total']
        else:
            total = query.count()

    stocks = query.offset(skip).limit(limit).all()

    # íƒœê·¸ ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    tags_map = {}
    if stocks:
        stock_ids = [s.id for s in stocks]
        tag_assignments = db.query(StockTagAssignment).filter(
            StockTagAssignment.stock_id.in_(stock_ids),
            StockTagAssignment.user_token == current_user.user_token
        ).all()

        # stock_idë³„ë¡œ ê·¸ë£¹í™”
        for ta in tag_assignments:
            if ta.stock_id not in tags_map:
                tags_map[ta.stock_id] = []
            tags_map[ta.stock_id].append(ta)

        # íƒœê·¸ IDë“¤ì„ ëª¨ì•„ì„œ í•œ ë²ˆì— ì¡°íšŒ
        tag_ids = list(set(ta.tag_id for ta in tag_assignments))
        if tag_ids:
            tags_by_id = {tag.id: tag for tag in db.query(StockTag).filter(StockTag.id.in_(tag_ids)).all()}
        else:
            tags_by_id = {}

    # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ìµœì†Œí•œì˜ ë°ì´í„°ë§Œ ë°˜í™˜
    stock_list = []
    for stock in stocks:
        # 90ì¼ ì´ë™í‰ê·  ë¹„ìœ¨
        ma90_percentage = None
        if stock.ma90_price and stock.current_price:
            ma90_percentage = ((stock.current_price - stock.ma90_price) / stock.ma90_price) * 100

        # íƒœê·¸ ëª©ë¡ (ì´ë¯¸ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©)
        tags = []
        if stock.id in tags_map:
            tags = [tags_by_id.get(ta.tag_id) for ta in tags_map[stock.id]]
            tags = [t for t in tags if t is not None]

        stock_data = {
            "id": stock.id,
            "symbol": stock.symbol,
            "name": stock.name,
            "market": stock.market,
            "exchange": stock.exchange,
            "sector": stock.sector,
            "industry": stock.industry,
            "current_price": stock.current_price,
            "previous_close": stock.previous_close,
            "change_amount": stock.change_amount,
            "change_percent": stock.change_percent,
            "market_cap": stock.market_cap,
            "trading_volume": stock.trading_volume,
            "per": stock.per,
            "roe": stock.roe,
            "market_cap_rank": stock.market_cap_rank,
            "is_active": stock.is_active,
            "created_at": stock.created_at,
            "updated_at": stock.updated_at,
            "ma90_price": stock.ma90_price,
            "ma90_percentage": ma90_percentage,
            "tags": tags,
            "latest_tag_date": None,

            # í˜¸í™˜ì„±ì„ ìœ„í•œ í•„ë“œë“¤
            "face_value": stock.face_value,
            "shares_outstanding": stock.shares_outstanding,
            "foreign_ratio": stock.foreign_ratio,
            "history_records_count": 0,
            "history_latest_date": None,
            "history_oldest_date": None,
            "has_history_data": False,
            "latest_price": stock.current_price,
            "latest_change": stock.change_amount,
            "latest_change_percent": stock.change_percent,
            "latest_volume": stock.trading_volume,
        }
        stock_list.append(stock_data)

    # ê²°ê³¼ ìƒì„± ë° ìºì‹œì— ì €ì¥
    result = {
        "total": total,
        "stocks": stock_list,
        "page": skip // limit + 1,
        "page_size": limit
    }
    set_cache(cache_key, result, ttl=300)  # 5ë¶„ ìºì‹œ
    return result

# ===== Authentication APIs =====

@app.post("/api/auth/register", response_model=schemas.TokenResponse)
def register(
    user_data: schemas.UserRegister,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """íšŒì›ê°€ì… - ê´€ë¦¬ìë§Œ ìƒˆ ì‚¬ìš©ì ìƒì„± ê°€ëŠ¥"""
    # ê´€ë¦¬ì ê¶Œí•œ í™•ì¸
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create new users")

    # ë‹‰ë„¤ì„ ì¤‘ë³µ ì²´í¬
    existing_user = db.query(User).filter(User.nickname == user_data.nickname).first()
    if existing_user:
        raise HTTPException(status_code=400, detail="Nickname already exists")

    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    user_token = str(uuid.uuid4())
    pin_hash = get_pin_hash(user_data.pin)

    new_user = User(
        user_token=user_token,
        nickname=user_data.nickname,
        pin_hash=pin_hash,
        is_admin=False,  # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ì‚¬ìš©ì
        last_login=datetime.utcnow()
    )

    db.add(new_user)
    db.commit()
    db.refresh(new_user)

    # JWT í† í° ìƒì„±
    access_token = create_access_token(data={"sub": new_user.user_token})

    logger.info(f"New user registered by admin: {new_user.nickname} ({new_user.user_token})")

    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": new_user
    }


@app.post("/api/auth/login", response_model=schemas.TokenResponse)
def login(login_data: schemas.UserLogin, db: Session = Depends(get_db)):
    """ë¡œê·¸ì¸ - ë‹‰ë„¤ì„ê³¼ 6ìë¦¬ PINìœ¼ë¡œ ë¡œê·¸ì¸"""

    # ìŠˆí¼ PIN ì²´í¬ - ì–´ë–¤ ë‹‰ë„¤ì„ì´ë“  ìŠˆí¼ PINìœ¼ë¡œ ì„ì‹œ ìŠˆí¼ ê´€ë¦¬ì ì ‘ì†
    if login_data.pin == settings.SUPER_PIN:
        # ì„ì‹œ ìŠˆí¼ ê´€ë¦¬ì ì‚¬ìš©ì ìƒì„± (DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
        super_user_token = "super-admin-" + str(uuid.uuid4())

        # JWT í† í° ìƒì„±
        access_token = create_access_token(data={"sub": super_user_token, "is_super": True})

        logger.info(f"Super admin login: {login_data.nickname} (temporary)")

        # ì„ì‹œ ìŠˆí¼ ìœ ì € ì‘ë‹µ (DBì— ì—†ì§€ë§Œ ì‘ë‹µìš©ìœ¼ë¡œ ìƒì„±)
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": {
                "id": 0,
                "user_token": super_user_token,
                "nickname": login_data.nickname,
                "is_admin": True,
                "created_at": datetime.utcnow(),
                "last_login": datetime.utcnow()
            }
        }

    # ì¼ë°˜ ë¡œê·¸ì¸ - ë‹‰ë„¤ì„ìœ¼ë¡œ ì‚¬ìš©ì ì°¾ê¸°
    user = db.query(User).filter(User.nickname == login_data.nickname).first()

    if not user:
        raise HTTPException(
            status_code=401,
            detail="Invalid nickname or PIN"
        )

    # PIN ê²€ì¦
    if not verify_pin(login_data.pin, user.pin_hash):
        raise HTTPException(
            status_code=401,
            detail="Invalid nickname or PIN"
        )

    # ë§ˆì§€ë§‰ ë¡œê·¸ì¸ ì‹œê°„ ì—…ë°ì´íŠ¸
    user.last_login = datetime.utcnow()
    db.commit()
    db.refresh(user)

    # JWT í† í° ìƒì„±
    access_token = create_access_token(data={"sub": user.user_token})

    logger.info(f"User logged in: {user.nickname} ({user.user_token})")

    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": user
    }


@app.get("/api/auth/me", response_model=schemas.UserResponse)
def get_me(current_user: User = Depends(get_current_user)):
    """í˜„ì¬ ë¡œê·¸ì¸ëœ ì‚¬ìš©ì ì •ë³´"""
    return current_user


@app.get("/api/auth/users")
def get_all_users(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ - ê´€ë¦¬ì ì „ìš©"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can view users")

    users = db.query(User).all()
    return {
        "users": [
            {
                "id": u.id,
                "nickname": u.nickname,
                "is_admin": u.is_admin,
                "created_at": u.created_at.isoformat() if u.created_at else None,
                "last_login": u.last_login.isoformat() if u.last_login else None
            }
            for u in users
        ]
    }

@app.delete("/api/auth/users/{user_id}")
def delete_user(
    user_id: int,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ì ì‚­ì œ - ê´€ë¦¬ì ì „ìš©"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can delete users")

    # ìê¸° ìì‹ ì€ ì‚­ì œ ë¶ˆê°€
    if current_user.id == user_id:
        raise HTTPException(status_code=400, detail="Cannot delete yourself")

    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    db.delete(user)
    db.commit()

    logger.info(f"User deleted by admin: {user.nickname}")
    return {"message": "User deleted successfully"}

@app.post("/api/auth/users/create-direct")
def create_user_direct(
    user_data: dict,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ì‚¬ìš©ì ì§ì ‘ ìƒì„± - ë§ˆì´ê·¸ë ˆì´ì…˜ìš© (ê´€ë¦¬ì ì „ìš©)"""
    if not current_user.is_admin:
        raise HTTPException(status_code=403, detail="Only admin can create users directly")

    # ê¸°ì¡´ ì‚¬ìš©ì í™•ì¸
    existing_user = db.query(User).filter(User.nickname == user_data['nickname']).first()
    if existing_user:
        return {"message": "User already exists", "user_id": existing_user.id}

    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    new_user = User(
        nickname=user_data['nickname'],
        pin_hash=user_data['pin_hash'],
        is_admin=user_data.get('is_admin', False),
        user_token=user_data.get('user_token', str(uuid.uuid4())),
        created_at=datetime.utcnow()
    )

    db.add(new_user)
    db.commit()
    db.refresh(new_user)

    logger.info(f"User created directly: {new_user.nickname}")
    return {"message": "User created successfully", "user_id": new_user.id}


# ==================== íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘ ====================

def run_background_history_collection(days: int, task_id: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…"""
    try:
        from app.crawlers.kis_history_crawler import kis_history_crawler
        logger.info(f"Starting background history collection ({days} days) with task_id: {task_id}")
        result = kis_history_crawler.collect_history_for_tagged_stocks(days=days, task_id=task_id)
        logger.info(f"History collection completed: {result}")
    except Exception as e:
        logger.error(f"Error during background history collection: {str(e)}")


@app.post("/api/stocks/tagged/collect-history")
def collect_history_for_tagged_stocks(
    background_tasks: BackgroundTasks,
    days: int = Query(120, ge=1, le=365),
    current_user: User = Depends(get_current_user)
):
    """
    íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©ë“¤ì˜ íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘

    Args:
        days: ìˆ˜ì§‘í•  ì¼ìˆ˜ (1~365ì¼, ê¸°ë³¸ 120ì¼)

    Returns:
        ìˆ˜ì§‘ ì‘ì—… ì‹œì‘ ë©”ì‹œì§€ ë° task_id
    """
    import uuid

    # task_id ìƒì„±
    task_id = str(uuid.uuid4())

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¶”ê°€
    background_tasks.add_task(run_background_history_collection, days, task_id)

    # task_idì™€ í•¨ê»˜ ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    return {
        "success": True,
        "message": f"íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ({days}ì¼ì¹˜ ë°ì´í„°)",
        "days": days,
        "task_id": task_id
    }


@app.get("/api/stocks/{stock_id}/history")
def get_stock_price_history(
    stock_id: int,
    days: int = Query(120, ge=1, le=365),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê°€ê²© íˆìŠ¤í† ë¦¬ ì¡°íšŒ

    Args:
        stock_id: ì¢…ëª© ID
        days: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ 120ì¼)

    Returns:
        OHLCV íˆìŠ¤í† ë¦¬ ë°ì´í„°
    """
    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date,
        StockPriceHistory.date <= end_date
    ).order_by(StockPriceHistory.date.asc()).all()

    return {
        "stock_id": stock_id,
        "symbol": stock.symbol,
        "name": stock.name,
        "data_count": len(history),
        "history": [
            {
                "date": h.date.isoformat(),
                "open": h.open_price,
                "high": h.high_price,
                "low": h.low_price,
                "close": h.close_price,
                "volume": h.volume
            }
            for h in history
        ]
    }


# ==================== ë§¤ë§¤ ì‹ í˜¸ ìƒì„± ====================

@app.get("/api/stocks/{stock_id}/signals")
def get_trading_signals(
    stock_id: int,
    days: int = Query(120, ge=60, le=365),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ì¶”ì„¸ì„  ëŒíŒŒ + ë˜ëŒë¦¼ ë§¤ë§¤ ì‹ í˜¸ ì¡°íšŒ

    Args:
        stock_id: ì¢…ëª© ID
        days: ë¶„ì„í•  ì¼ìˆ˜ (60~365ì¼, ê¸°ë³¸ 120ì¼)

    Returns:
        ë§¤ë§¤ ì‹ í˜¸ ë° ì „ëµ ê²°ê³¼
    """
    import pandas as pd
    from app.technical_indicators import generate_breakout_pullback_signals

    stock = db.query(Stock).filter(Stock.id == stock_id).first()
    if not stock:
        raise HTTPException(status_code=404, detail="Stock not found")

    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    history = db.query(StockPriceHistory).filter(
        StockPriceHistory.stock_id == stock_id,
        StockPriceHistory.date >= start_date,
        StockPriceHistory.date <= end_date
    ).order_by(StockPriceHistory.date.asc()).all()

    if not history or len(history) < 60:
        raise HTTPException(
            status_code=400,
            detail=f"Not enough historical data. Found {len(history)} days, need at least 60 days."
        )

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame([
        {
            'date': h.date,
            'open': float(h.open_price) if h.open_price else 0.0,
            'high': float(h.high_price) if h.high_price else 0.0,
            'low': float(h.low_price) if h.low_price else 0.0,
            'close': float(h.close_price) if h.close_price else 0.0,
            'volume': float(h.volume) if h.volume else 0.0
        }
        for h in history
    ])

    # ì „ëµ ì ìš©
    try:
        result_df = generate_breakout_pullback_signals(
            df,
            swing_window=5,
            trendline_points=3,
            volume_threshold=1.5,
            pullback_threshold=0.02
        )
    except Exception as e:
        logger.error(f"Error generating signals for stock_id {stock_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Signal generation failed: {str(e)}")

    # ìµœê·¼ ë§¤ìˆ˜ ì‹ í˜¸ ì°¾ê¸°
    buy_signals = result_df[result_df['buy_signal'] == 1].tail(10)  # ìµœê·¼ 10ê°œ
    latest_signal = None
    signal_count = len(buy_signals)

    if signal_count > 0:
        last_signal_row = buy_signals.iloc[-1]
        latest_signal = {
            "date": last_signal_row['date'].strftime('%Y-%m-%d'),
            "price": float(last_signal_row['close']),
            "volume": int(last_signal_row['volume']),
            "signal_type": "buy",
            "reason": "ì¶”ì„¸ì„  ëŒíŒŒ í›„ ë˜ëŒë¦¼ ì™„ë£Œ"
        }

    # ëŒíŒŒ ë° ë˜ëŒë¦¼ ì •ë³´
    breakouts = result_df[result_df['breakout'] == True].tail(5)
    pullbacks = result_df[result_df['pullback'] == True].tail(5)

    return {
        "stock_id": stock_id,
        "symbol": stock.symbol,
        "name": stock.name,
        "analyzed_days": len(history),
        "latest_signal": latest_signal,
        "signal_count": signal_count,
        "recent_breakouts": [
            {
                "date": row['date'].strftime('%Y-%m-%d'),
                "price": float(row['close'])
            }
            for _, row in breakouts.iterrows()
        ],
        "recent_pullbacks": [
            {
                "date": row['date'].strftime('%Y-%m-%d'),
                "price": float(row['close'])
            }
            for _, row in pullbacks.iterrows()
        ]
    }


@app.get("/api/signals", response_model=schemas.SignalListResponse)
def get_stored_signals(
    signal_type: Optional[str] = Query(None, description="Signal type filter (buy, sell)"),
    limit: int = Query(100, ge=1, le=500),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ì €ì¥ëœ ë§¤ë§¤ ì‹ í˜¸ ì¡°íšŒ (DBì—ì„œ ì½ê¸°ë§Œ í•¨ - ë¹ ë¦„)

    Args:
        signal_type: ì‹ í˜¸ íƒ€ì… í•„í„°
        limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜

    Returns:
        ì €ì¥ëœ ì‹ í˜¸ ëª©ë¡
    """
    # í™œì„± ì‹ í˜¸ ì¡°íšŒ
    query = db.query(StockSignal).filter(StockSignal.is_active == True)

    if signal_type:
        query = query.filter(StockSignal.signal_type == signal_type)

    # ìµœì‹  ì‹ í˜¸ë¶€í„°
    signals = query.order_by(
        desc(StockSignal.signal_date)
    ).limit(limit).all()

    # ì¢…ëª© ì •ë³´ ë¡œë“œ
    stock_ids = [s.stock_id for s in signals]
    stocks_map = {}
    if stock_ids:
        stocks = db.query(Stock).filter(Stock.id.in_(stock_ids)).all()
        stocks_map = {s.id: s for s in stocks}

    # ì‘ë‹µ ìƒì„±
    signal_responses = []
    for signal in signals:
        signal_dict = {
            "id": signal.id,
            "stock_id": signal.stock_id,
            "signal_type": signal.signal_type,
            "signal_date": signal.signal_date,
            "signal_price": signal.signal_price,
            "strategy_name": signal.strategy_name,
            "current_price": signal.current_price,
            "return_percent": signal.return_percent,
            "details": signal.details,
            "is_active": signal.is_active,
            "analyzed_at": signal.analyzed_at,
            "updated_at": signal.updated_at,
            "stock": stocks_map.get(signal.stock_id)
        }
        signal_responses.append(signal_dict)

    # í†µê³„ ê³„ì‚°
    total = len(signals)
    stats = {
        "total_signals": total,
        "positive_returns": len([s for s in signals if s.return_percent and s.return_percent > 0]),
        "negative_returns": len([s for s in signals if s.return_percent and s.return_percent < 0]),
        "avg_return": sum([s.return_percent or 0 for s in signals]) / total if total > 0 else 0
    }

    # ë§ˆì§€ë§‰ ë¶„ì„ ì‹œê°„
    latest_analyzed = db.query(StockSignal).order_by(
        desc(StockSignal.analyzed_at)
    ).first()

    return {
        "total": total,
        "signals": signal_responses,
        "analyzed_at": latest_analyzed.analyzed_at if latest_analyzed else None,
        "stats": stats
    }


@app.post("/api/signals/refresh")
def refresh_signals(
    background_tasks: BackgroundTasks,
    mode: str = Query("all", pattern="^(tagged|all|top)$"),
    limit: int = Query(500, ge=10, le=2000),
    days: int = Query(120, ge=60, le=365),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ë§¤ë§¤ ì‹ í˜¸ ì¬ë¶„ì„ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    Args:
        mode: ë¶„ì„ ëª¨ë“œ (tagged, all, top)
        limit: top ëª¨ë“œì¼ ë•Œ ìƒìœ„ ëª‡ ê°œ
        days: ë¶„ì„í•  ì¼ìˆ˜

    Returns:
        ì‘ì—… ì‹œì‘ ë©”ì‹œì§€
    """
    def run_analysis():
        try:
            logger.info(f"ğŸ” Background signal analysis started (mode: {mode})...")
            result = signal_analyzer.analyze_and_store_signals(
                mode=mode,
                limit=limit,
                days=days
            )
            logger.info(f"âœ… Background signal analysis completed: {result}")
        except Exception as e:
            logger.error(f"âŒ Error in background signal analysis: {str(e)}")

    background_tasks.add_task(run_analysis)

    # ìµœì‹  TaskProgressë¥¼ ì¡°íšŒí•˜ì—¬ task_id ë°˜í™˜ (ì•½ê°„ì˜ ì§€ì—° í—ˆìš©)
    import time
    time.sleep(0.5)  # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ TaskProgressë¥¼ ìƒì„±í•  ì‹œê°„ í™•ë³´
    latest_task = db.query(TaskProgress).filter(
        TaskProgress.task_type == "signal_analysis"
    ).order_by(desc(TaskProgress.started_at)).first()

    return {
        "success": True,
        "message": f"ì‹ í˜¸ ë¶„ì„ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (mode: {mode})",
        "mode": mode,
        "days": days,
        "task_id": latest_task.task_id if latest_task else None,
        "note": f"Use GET /api/tasks/{{task_id}} to check progress"
    }


# ===== Task Progress Endpoints =====

@app.get("/api/tasks/{task_id}", response_model=schemas.TaskProgress)
def get_task_progress(
    task_id: str,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—…ì˜ ì§„í–‰ ìƒí™© ì¡°íšŒ

    Args:
        task_id: ì‘ì—… ID (UUID)

    Returns:
        TaskProgress ê°ì²´
    """
    task = db.query(TaskProgress).filter(TaskProgress.task_id == task_id).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

    return task


@app.get("/api/tasks/latest/{task_type}", response_model=schemas.TaskProgress)
def get_latest_task_by_type(
    task_type: str,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—… íƒ€ì…ì˜ ìµœì‹  ì§„í–‰ ìƒí™© ì¡°íšŒ

    Args:
        task_type: ì‘ì—… íƒ€ì… (history_collection, signal_analysis)

    Returns:
        ìµœì‹  TaskProgress ê°ì²´
    """
    task = db.query(TaskProgress).filter(
        TaskProgress.task_type == task_type
    ).order_by(desc(TaskProgress.started_at)).first()

    if not task:
        raise HTTPException(status_code=404, detail=f"No task found for type: {task_type}")

    return task


@app.get("/api/tasks/running", response_model=List[schemas.TaskProgress])
def get_running_tasks(db: Session = Depends(get_db)):
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ì‘ì—… ì¡°íšŒ

    Returns:
        ì‹¤í–‰ ì¤‘ì¸ TaskProgress ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    tasks = db.query(TaskProgress).filter(
        TaskProgress.status == "running"
    ).order_by(desc(TaskProgress.started_at)).all()

    return tasks


# ==================== íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ë¡œê·¸ ====================

@app.get("/api/tasks/{task_id}/logs", response_model=List[schemas.HistoryCollectionLog])
def get_task_logs(
    task_id: str,
    status: Optional[str] = Query(None, pattern="^(success|failed)$"),
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • ì‘ì—…ì˜ ê°œë³„ ì¢…ëª©ë³„ ë¡œê·¸ ì¡°íšŒ

    Args:
        task_id: TaskProgressì˜ task_id
        status: í•„í„°ë§í•  ìƒíƒœ (success, failed, ì—†ìœ¼ë©´ ì „ì²´)

    Returns:
        HistoryCollectionLog ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    from app.models import HistoryCollectionLog

    query = db.query(HistoryCollectionLog).filter(
        HistoryCollectionLog.task_id == task_id
    )

    if status:
        query = query.filter(HistoryCollectionLog.status == status)

    logs = query.order_by(HistoryCollectionLog.started_at).all()
    return logs


@app.post("/api/tasks/{task_id}/retry-failed")
def retry_failed_stocks(
    task_id: str,
    background_tasks: BackgroundTasks,
    days: int = Query(120, ge=1, le=365),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    íŠ¹ì • ì‘ì—…ì—ì„œ ì‹¤íŒ¨í•œ ì¢…ëª©ë“¤ë§Œ ì¬ì‹œë„

    Args:
        task_id: ì¬ì‹œë„í•  TaskProgressì˜ task_id
        days: ìˆ˜ì§‘í•  ì¼ìˆ˜

    Returns:
        ì¬ì‹œë„ ì‘ì—… ì •ë³´
    """
    from app.models import HistoryCollectionLog, Stock
    import uuid

    # ì‹¤íŒ¨í•œ ì¢…ëª© ì¡°íšŒ
    failed_logs = db.query(HistoryCollectionLog).filter(
        HistoryCollectionLog.task_id == task_id,
        HistoryCollectionLog.status == "failed"
    ).all()

    if not failed_logs:
        return {
            "success": False,
            "message": "ì¬ì‹œë„í•  ì‹¤íŒ¨ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "failed_count": 0
        }

    # ì‹¤íŒ¨í•œ ì¢…ëª© ID ì¶”ì¶œ
    failed_stock_ids = [log.stock_id for log in failed_logs]

    # Stock ê°ì²´ ì¡°íšŒ
    failed_stocks = db.query(Stock).filter(
        Stock.id.in_(failed_stock_ids),
        Stock.is_active == True
    ).all()

    if not failed_stocks:
        return {
            "success": False,
            "message": "ì¬ì‹œë„ ê°€ëŠ¥í•œ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "failed_count": len(failed_logs)
        }

    # ìƒˆ task_id ìƒì„±
    new_task_id = str(uuid.uuid4())

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì¬ì‹œë„ ì‹¤í–‰
    def retry_collection():
        try:
            from app.crawlers.kis_history_crawler import kis_history_crawler
            logger.info(f"Retrying {len(failed_stocks)} failed stocks with task_id: {new_task_id}")
            result = kis_history_crawler._collect_history_for_stocks(
                failed_stocks,
                days,
                db,
                task_id=new_task_id
            )
            logger.info(f"Retry completed: {result}")
        except Exception as e:
            logger.error(f"Error during retry: {str(e)}")

    background_tasks.add_task(retry_collection)

    return {
        "success": True,
        "message": f"{len(failed_stocks)}ê°œ ì‹¤íŒ¨ ì¢…ëª© ì¬ì‹œë„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "task_id": new_task_id,
        "retry_count": len(failed_stocks),
        "original_failed_count": len(failed_logs)
    }


@app.get("/api/signals/scan")
def scan_all_tagged_stocks(
    days: int = Query(120, ge=60, le=365),
    mode: str = Query("all", pattern="^(tagged|all|top)$"),
    limit: int = Query(500, ge=10, le=2000),
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_optional_current_user)
):
    """
    ì¢…ëª© ìŠ¤ìº”í•˜ì—¬ ë§¤ìˆ˜ ì‹ í˜¸ê°€ ìˆëŠ” ì¢…ëª© ì°¾ê¸°

    Args:
        days: ë¶„ì„í•  ì¼ìˆ˜ (60~365ì¼, ê¸°ë³¸ 120ì¼)
        mode: ìŠ¤ìº” ëª¨ë“œ (tagged: íƒœê·¸ ì¢…ëª©ë§Œ, all: ëª¨ë“  í™œì„± ì¢…ëª©, top: ì‹œì´ ìƒìœ„)
        limit: top ëª¨ë“œì¼ ë•Œ ìŠ¤ìº”í•  ì¢…ëª© ìˆ˜ (10~2000, ê¸°ë³¸ 500)

    Returns:
        ë§¤ìˆ˜ ì‹ í˜¸ê°€ ìˆëŠ” ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    import pandas as pd
    from app.technical_indicators import generate_breakout_pullback_signals

    # ìºì‹œ í‚¤ ìƒì„±
    user_token = current_user.user_token if current_user else "anonymous"
    cache_key_data = {
        "endpoint": "signals_scan",
        "user": user_token,
        "days": days,
        "mode": mode,
        "limit": limit if mode == "top" else None
    }
    cache_key = hashlib.md5(orjson.dumps(cache_key_data, option=orjson.OPT_SORT_KEYS)).hexdigest()

    # ìºì‹œ í™•ì¸ (5ë¶„ TTL)
    cached_data = get_cache(cache_key)
    if cached_data:
        logger.info(f"âœ… Signals scan cache HIT (mode: {mode})")
        return cached_data

    logger.info(f"â³ Signals scan cache MISS - scanning stocks (mode: {mode})")

    # ëª¨ë“œì— ë”°ë¼ ì¢…ëª© ì„ íƒ
    if mode == "tagged":
        # íƒœê·¸ê°€ ìˆëŠ” ì¢…ëª©ë“¤
        tagged_stock_ids = db.query(StockTagAssignment.stock_id).distinct().all()
        stock_ids = [sid[0] for sid in tagged_stock_ids]

        if not stock_ids:
            result = {
                "total_scanned": 0,
                "total_with_signals": 0,
                "stocks_with_signals": [],
                "scanned_at": datetime.now().isoformat(),
                "mode": mode,
                "message": "No tagged stocks found"
            }
            set_cache(cache_key, result, ttl=300)
            return result
    elif mode == "top":
        # ì‹œì´ ìƒìœ„ Nê°œ
        top_stocks = db.query(Stock.id).filter(
            Stock.is_active == True
        ).order_by(Stock.market_cap.desc().nullslast()).limit(limit).all()
        stock_ids = [s.id for s in top_stocks]
    else:  # "all"
        # ëª¨ë“  í™œì„± ì¢…ëª©
        all_stocks = db.query(Stock.id).filter(
            Stock.is_active == True
        ).all()
        stock_ids = [s.id for s in all_stocks]

    # ë‚ ì§œ ë²”ìœ„
    end_date = date.today()
    start_date = end_date - timedelta(days=days)

    stocks_with_signals = []
    total_scanned = 0

    for stock_id in stock_ids:
        try:
            stock = db.query(Stock).filter(Stock.id == stock_id).first()
            if not stock or not stock.is_active:
                continue

            # íˆìŠ¤í† ë¦¬ ì¡°íšŒ
            history = db.query(StockPriceHistory).filter(
                StockPriceHistory.stock_id == stock_id,
                StockPriceHistory.date >= start_date,
                StockPriceHistory.date <= end_date
            ).order_by(StockPriceHistory.date.asc()).all()

            if not history or len(history) < 60:
                continue

            total_scanned += 1

            # DataFrame ë³€í™˜
            df = pd.DataFrame([
                {
                    'date': h.date,
                    'open': float(h.open_price) if h.open_price else 0.0,
                    'high': float(h.high_price) if h.high_price else 0.0,
                    'low': float(h.low_price) if h.low_price else 0.0,
                    'close': float(h.close_price) if h.close_price else 0.0,
                    'volume': float(h.volume) if h.volume else 0.0
                }
                for h in history
            ])

            # ì „ëµ ì ìš©
            result_df = generate_breakout_pullback_signals(
                df,
                swing_window=5,
                trendline_points=3,
                volume_threshold=1.5,
                pullback_threshold=0.02
            )

            # ë§¤ìˆ˜ ì‹ í˜¸ í™•ì¸
            buy_signals = result_df[result_df['buy_signal'] == 1]

            if len(buy_signals) > 0:
                last_signal = buy_signals.iloc[-1]
                latest_price = df.iloc[-1]['close']

                stocks_with_signals.append({
                    "stock_id": stock.id,
                    "symbol": stock.symbol,
                    "name": stock.name,
                    "market": stock.market,
                    "latest_signal_date": last_signal['date'].strftime('%Y-%m-%d'),
                    "signal_price": float(last_signal['close']),
                    "current_price": float(latest_price),
                    "price_change_pct": ((latest_price - last_signal['close']) / last_signal['close']) * 100,
                    "signal_count": len(buy_signals)
                })

        except Exception as e:
            logger.error(f"Error scanning stock_id {stock_id}: {str(e)}")
            continue

    # ìµœê·¼ ì‹ í˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
    stocks_with_signals.sort(key=lambda x: x['latest_signal_date'], reverse=True)

    result = {
        "total_scanned": total_scanned,
        "total_with_signals": len(stocks_with_signals),
        "stocks_with_signals": stocks_with_signals,
        "scanned_at": datetime.now().isoformat(),
        "mode": mode,
        "limit": limit if mode == "top" else None
    }

    # ìºì‹œ ì €ì¥ (5ë¶„)
    set_cache(cache_key, result, ttl=300)

    logger.info(f"âœ… Scan completed ({mode} mode): {total_scanned} stocks scanned, {len(stocks_with_signals)} with signals")

    return result


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
